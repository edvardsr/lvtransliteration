{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEq02TlSEZj4"
      },
      "source": [
        "# Main Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLYYoHNOEZj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c752c7c-9038-4257-b236-150478805fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipatok\n",
            "  Downloading ipatok-0.4.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting pykakasi\n",
            "  Downloading pykakasi-2.2.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pinyin_jyutping_sentence\n",
            "  Downloading pinyin_jyutping_sentence-1.3.tar.gz (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting korean_romanizer\n",
            "  Downloading korean_romanizer-0.25.1-py3-none-any.whl (18 kB)\n",
            "Collecting jaconv (from pykakasi)\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from pykakasi)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from pinyin_jyutping_sentence) (0.42.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pykakasi) (1.14.1)\n",
            "Building wheels for collected packages: pinyin_jyutping_sentence, jaconv\n",
            "  Building wheel for pinyin_jyutping_sentence (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin_jyutping_sentence: filename=pinyin_jyutping_sentence-1.3-py3-none-any.whl size=12492668 sha256=2db0468680fbd8f2b4f62a53ce4b44898d583635b557575e527a1e802f247134\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/b7/09/99fe76af71b53ba2d93146f4aba014579bd7d08ec8b3ad8754\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16416 sha256=86b42aa7f8f5554040e9d0bcee82c2d3e619d801e671c5f6dade14235097a848\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/8f/2e/a730bf1fca05b33e532d5d91dabdf406c9b718ec85b01b1b54\n",
            "Successfully built pinyin_jyutping_sentence jaconv\n",
            "Installing collected packages: korean_romanizer, jaconv, ipatok, pinyin_jyutping_sentence, deprecated, pykakasi\n",
            "Successfully installed deprecated-1.2.14 ipatok-0.4.2 jaconv-0.3.4 korean_romanizer-0.25.1 pinyin_jyutping_sentence-1.3 pykakasi-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install ipatok pykakasi pinyin_jyutping_sentence korean_romanizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVH-kSeSEZj6"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "from ipatok import tokenise\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import shutil\n",
        "import copy\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import pinyin_jyutping_sentence\n",
        "import pykakasi\n",
        "from korean_romanizer.romanizer import Romanizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "import numbers\n",
        "import csv\n",
        "\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmR1Jln5EZj7"
      },
      "source": [
        "# Local Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jydTPps2EZj7"
      },
      "outputs": [],
      "source": [
        "LC = {\n",
        "    'is_initialized': False,\n",
        "    'is_colab': True if os.getenv(\"COLAB_RELEASE_TAG\") else False,\n",
        "    'colab_auto_quit': True,\n",
        "    'to_train': True,\n",
        "    'lang': 'ja',\n",
        "    'to_load': False,\n",
        "    'to_load_model': '2024-04-20_21-50-55',\n",
        "    'to_save': True,\n",
        "    'drive_mount': '/content/drive',\n",
        "    'drive_root': '/content/drive/My Drive/seq2seq/',\n",
        "    'src_data': 'training-v12-full.zip',\n",
        "    'src_testing': 'testing-v12.zip',\n",
        "    'batch_size': int(5800 * (1280 / (1536 + 40))), # can be 'auto' or a number\n",
        "    'epoch_goal': 500,\n",
        "    'data_root': 'data',\n",
        "    'testing_root': 'testing',\n",
        "    'model_root': 'models',\n",
        "    'model_path': '',\n",
        "    'model_export_path': '',\n",
        "    'default_model_config': {\n",
        "        'lang': False,\n",
        "        'hidden_size': 1536,\n",
        "        'max_length': 17,\n",
        "        'epoch_count': 0,\n",
        "        'last_trained_at': False,\n",
        "        'created_at': False,\n",
        "        'title': 'ja-v12-1536-full',\n",
        "        'testing_high_total': 0,\n",
        "        'testing_high_lv': 0,\n",
        "        'testing_high_median': 0,\n",
        "        'testing_high_epoch': 0,\n",
        "        'testing_saved_total': 0,\n",
        "        'testing_saved_lv': 0,\n",
        "        'testing_latest_total': 0,\n",
        "        'testing_latest_lv': 0,\n",
        "        'testing_latest_median': 0,\n",
        "        'testing_latest_epoch': 0,\n",
        "        'testing_max_total': 0,\n",
        "        'verify_high_total': 0,\n",
        "        'verify_high_median': 0,\n",
        "        'verify_high_epoch': 0,\n",
        "        'verify_saved_total': 0,\n",
        "        'verify_latest_total': 0,\n",
        "        'verify_latest_median': 0,\n",
        "        'verify_latest_epoch': 0,\n",
        "        'verify_max_total': 0,\n",
        "    },\n",
        "    'model': False\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2R2Za-nEZj8"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED6aEkUOEZj8"
      },
      "source": [
        "## Lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl417qlnEZj8"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, isIpa = False):\n",
        "        self.token2index = {}\n",
        "        self.token2count = {}\n",
        "        self.index2token = {0: \"?\", 1: \"EOS\" }\n",
        "        self.n_tokens = len(self.index2token.keys())\n",
        "        self.isIpa = isIpa\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if self.isIpa:\n",
        "            word, language = splitInput(word)\n",
        "            if language:\n",
        "                self.addToken(language)\n",
        "\n",
        "            word = standardiseIpa(word)\n",
        "\n",
        "        for token in word:\n",
        "            self.addToken(token)\n",
        "\n",
        "    def addToken(self, token):\n",
        "        if token not in self.token2index:\n",
        "            self.token2index[token] = self.n_tokens\n",
        "            self.token2count[token] = 1\n",
        "            self.index2token[self.n_tokens] = token\n",
        "            self.n_tokens += 1\n",
        "        else:\n",
        "            self.token2count[token] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrNAe3J2I-uU"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLUasu-4I-uU"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlwr36wEEZj8"
      },
      "source": [
        "## Attention (Bahdanau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM83aftSEZj9"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzF7cX93I-uU"
      },
      "source": [
        "## Decoder (with attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjNm1sakI-uV"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, maxWordLength, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.maxWordLength = maxWordLength\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(self.maxWordLength):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp5A1n0HEZj9"
      },
      "source": [
        "# Initialization Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGp620D4EZj9"
      },
      "source": [
        "## Get Date Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6SI2tkkEZj-"
      },
      "outputs": [],
      "source": [
        "def getDateTime():\n",
        "    return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QvlgChJEZj-"
      },
      "source": [
        "## Mount COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWO_LZiHEZj-"
      },
      "outputs": [],
      "source": [
        "def mountColab():\n",
        "  drive.mount(LC['drive_mount'])\n",
        "\n",
        "  !cp \"{os.path.join(LC['drive_root'], LC['src_data'])}\" \"{os.path.join(LC['data_root'], LC['src_data'])}\"\n",
        "  !cp \"{os.path.join(LC['drive_root'], LC['src_testing'])}\" \"{os.path.join(LC['testing_root'], LC['src_testing'])}\"\n",
        "\n",
        "  !unzip -o \"{os.path.join(LC['data_root'], LC['src_data'])}\" -d \"{os.path.join(LC['data_root'])}\"\n",
        "  !unzip -o \"{os.path.join(LC['testing_root'], LC['src_testing'])}\" -d \"{os.path.join(LC['testing_root'])}\"\n",
        "  !rm \"{os.path.join(LC['data_root'], LC['src_data'])}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOyd8QaKEZj-"
      },
      "source": [
        "## Get Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DV6mCeNI-uS"
      },
      "outputs": [],
      "source": [
        "def getPairs(filename):\n",
        "    # Read the file and split into lines\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs\n",
        "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVdF5sisIaZs"
      },
      "source": [
        "## Get Testing File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XejdeiGgIaZt"
      },
      "outputs": [],
      "source": [
        "def getTestingFile(filename):\n",
        "    # Read the file and split into lines\n",
        "    content = open(filename, encoding='utf-8').read().strip()\n",
        "\n",
        "    return json.loads(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfsuXKo0EZj-"
      },
      "source": [
        "## Create Lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAk2ISpqEZj-"
      },
      "outputs": [],
      "source": [
        "def createLang(pairs, pairIndex, isIpa):\n",
        "    lang = Lang(isIpa)\n",
        "\n",
        "    for pair in pairs:\n",
        "        lang.addWord(pair[pairIndex])\n",
        "\n",
        "    return lang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkZZfrc6EZj_"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lMEO4XuEZj_"
      },
      "outputs": [],
      "source": [
        "def createModel():\n",
        "    config = copy.copy(LC['default_model_config'])\n",
        "    config['created_at'] = getDateTime()\n",
        "    config['lang'] = LC['lang']\n",
        "\n",
        "    LC['model_path'] = os.path.join(LC['model_root'], config['lang'], config['title'])\n",
        "    LC['model_export_path'] = os.path.join(LC['drive_root'], LC['model_root'], config['lang'], config['title'] + '.zip')\n",
        "\n",
        "    pairs = getPairs(os.path.join(LC['data_root'], config['lang'] + '.tsv'))\n",
        "\n",
        "    if LC['batch_size'] == 'auto':\n",
        "        LC['batch_size'] = len(pairs)\n",
        "\n",
        "    inputLang = createLang(pairs, 0, True)\n",
        "    outputLang = createLang(pairs, 1, False)\n",
        "\n",
        "    encoder = EncoderRNN(inputLang.n_tokens, config['hidden_size']).to(device)\n",
        "    decoder = AttnDecoderRNN(config['hidden_size'], outputLang.n_tokens, config['max_length']).to(device)\n",
        "    encoder = encoder.to(memory_format=torch.channels_last)\n",
        "    decoder = decoder.to(memory_format=torch.channels_last)\n",
        "\n",
        "    testing = getTestingFile(os.path.join(LC['testing_root'], 'testing_%s.json' % config['lang']))\n",
        "    config['testing_max_total'] = testing[2][len(testing[2]) - 1]\n",
        "    verifyData = getTestingFile(os.path.join(LC['testing_root'], 'verify_%s.json' % config['lang']))\n",
        "    config['verify_max_total'] = verifyData[2][len(verifyData[2]) - 1]\n",
        "\n",
        "    romanized = []\n",
        "\n",
        "    for item in testing[2]:\n",
        "      if not isinstance(item, numbers.Number):\n",
        "        romanized.append(romanize(config['lang'], item))\n",
        "\n",
        "    testing.append(romanized)\n",
        "\n",
        "    return {\n",
        "        'config': config,\n",
        "        'input_lang': inputLang,\n",
        "        'output_lang': outputLang,\n",
        "        'pairs': pairs,\n",
        "        'encoder': encoder,\n",
        "        'decoder': decoder,\n",
        "        'loss_plot': [],\n",
        "        'testing': testing,\n",
        "        'verify': verifyData\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em237Hc6EZj_"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiBv2dhIEZj_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def loadModel():\n",
        "    LC['model_export_path'] = os.path.join(LC['drive_root'], LC['model_root'], LC['lang'], LC['to_load_model'] + '.zip')\n",
        "    LC['model_path'] = os.path.join(LC['model_root'], LC['lang'], LC['to_load_model'])\n",
        "\n",
        "    !mkdir -p \"{LC['model_path']}\"\n",
        "    if LC['is_colab']:\n",
        "        !unzip -o \"{LC['model_export_path']}\" -d \"{LC['model_path']}\"\n",
        "\n",
        "    config = False\n",
        "    with open(os.path.join(LC['model_path'], 'config.json'), 'r') as f:\n",
        "        config = json.load(f)\n",
        "    LC['lang'] = config['lang']\n",
        "\n",
        "    loss_plot = False\n",
        "    with open(os.path.join(LC['model_path'], 'loss_plot.json'), 'r') as f:\n",
        "        loss_plot = json.load(f)\n",
        "\n",
        "    pairs = getPairs(os.path.join(LC['model_path'], 'pairs.tsv'))\n",
        "\n",
        "    inputLang = False\n",
        "    with open(os.path.join(LC['model_path'], 'input_lang.pickle'), 'rb') as f:\n",
        "        inputLang = pickle.load(f)\n",
        "\n",
        "    outputLang = False\n",
        "    with open(os.path.join(LC['model_path'], 'output_lang.pickle'), 'rb') as f:\n",
        "        outputLang = pickle.load(f)\n",
        "\n",
        "    encoder = EncoderRNN(inputLang.n_tokens, config['hidden_size']).to(device)\n",
        "    decoder = AttnDecoderRNN(config['hidden_size'], outputLang.n_tokens, config['max_length']).to(device)\n",
        "\n",
        "    encoder.load_state_dict(torch.load(os.path.join(LC['model_path'], 'encoder.pth')))\n",
        "    decoder.load_state_dict(torch.load(os.path.join(LC['model_path'], 'decoder.pth')))\n",
        "    encoder = encoder.to(memory_format=torch.channels_last)\n",
        "    decoder = decoder.to(memory_format=torch.channels_last)\n",
        "\n",
        "    testing = False\n",
        "    with open(os.path.join(LC['model_path'], 'testing.json')) as f:\n",
        "        testing = json.load(f)\n",
        "\n",
        "    LC['model']['config']['testing_max_total'] = testing[2][len(testing[2]) - 1]\n",
        "\n",
        "    verifyData = False\n",
        "    with open(os.path.join(LC['model_path'], 'verify.json')) as f:\n",
        "        verifyData = json.load(f)\n",
        "\n",
        "    LC['model']['config']['verify_max_total'] = verifyData[2][len(verifyData[2]) - 1]\n",
        "\n",
        "    return {\n",
        "        'config': config,\n",
        "        'input_lang': inputLang,\n",
        "        'output_lang': outputLang,\n",
        "        'pairs': pairs,\n",
        "        'encoder': encoder,\n",
        "        'decoder': decoder,\n",
        "        'loss_plot': loss_plot,\n",
        "        'testing': testing,\n",
        "        'verify': verifyData\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E73D6DAtEZj_"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz_A1RJlEZkA"
      },
      "outputs": [],
      "source": [
        "def saveModel():\n",
        "    print('Saving...')\n",
        "    !mkdir -p \"{LC['model_path']}\"\n",
        "\n",
        "    LC['model']['config']['testing_saved_total'] = LC['model']['config']['testing_latest_total']\n",
        "    LC['model']['config']['testing_saved_lv'] = LC['model']['config']['testing_latest_lv']\n",
        "    LC['model']['config']['verify_saved_total'] = LC['model']['config']['verify_latest_total']\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'config.json'), 'w') as f:\n",
        "        json.dump(LC['model']['config'], f)\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'loss_plot.json'), 'w') as f:\n",
        "        json.dump(LC['model']['loss_plot'], f)\n",
        "\n",
        "    lines = []\n",
        "    for pair in LC['model']['pairs']:\n",
        "        lines.append(\"\\t\".join(pair))\n",
        "    with open(os.path.join(LC['model_path'], 'pairs.tsv'), 'w') as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'input_lang.pickle'), 'wb') as f:\n",
        "        pickle.dump(LC['model']['input_lang'], f)\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'output_lang.pickle'), 'wb') as f:\n",
        "        pickle.dump(LC['model']['output_lang'], f)\n",
        "\n",
        "    torch.save(LC['model']['encoder'].state_dict(), os.path.join(LC['model_path'], 'encoder.pth'))\n",
        "    torch.save(LC['model']['decoder'].state_dict(), os.path.join(LC['model_path'], 'decoder.pth'))\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'testing.json'), 'w') as f:\n",
        "        json.dump(LC['model']['testing'], f)\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'verify.json'), 'w') as f:\n",
        "        json.dump(LC['model']['verify'], f)\n",
        "\n",
        "    df = pd.DataFrame(LC['model']['testing'])\n",
        "    df.to_csv(os.path.join(LC['model_path'], 'testing.csv'), quoting=csv.QUOTE_NONNUMERIC)\n",
        "\n",
        "    df = pd.DataFrame(LC['model']['verify'])\n",
        "    df.to_csv(os.path.join(LC['model_path'], 'verify.csv'), quoting=csv.QUOTE_NONNUMERIC)\n",
        "\n",
        "    if LC['is_colab']:\n",
        "        !zip -q \"{LC['model_export_path']}\" -j \"{os.path.join('.', LC['model_path'])}\"/*\n",
        "\n",
        "    print('Saved!')\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkjmrV7aEZkA"
      },
      "source": [
        "## Standardise IPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGxerzZwEZkA"
      },
      "outputs": [],
      "source": [
        "def standardiseIpa(word):\n",
        "    return ''.join(tokenise(word, strict=False, replace=True, diphthongs=False, tones=False, unknown=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QooT_7bGEZkA"
      },
      "source": [
        "## Split Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itBY8ib9EZkA"
      },
      "outputs": [],
      "source": [
        "def splitInput(inputWord):\n",
        "    word = ''\n",
        "    language = False\n",
        "    if len(inputWord) > 2:\n",
        "        if inputWord[2] == \"_\":\n",
        "            language = inputWord[:3]\n",
        "            word = inputWord[3:]\n",
        "        else:\n",
        "            word = inputWord\n",
        "    else:\n",
        "        word = inputWord\n",
        "\n",
        "    return word, language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXauv9YtEZkB"
      },
      "source": [
        "## Tokenize Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM9zBBJlEZkB"
      },
      "outputs": [],
      "source": [
        "def tokenizeWord(lang, word):\n",
        "    tokens = []\n",
        "    if lang.isIpa:\n",
        "      word, language = splitInput(word)\n",
        "\n",
        "      if language:\n",
        "          tokens.append(language)\n",
        "\n",
        "      word = ''.join(tokenise(word, strict=False, replace=True, diphthongs=False, tones=False, unknown=False))\n",
        "    for token in word:\n",
        "        tokens.append(token)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QbGT71EZkB"
      },
      "source": [
        "## Get Indexes From Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqLiypl0EZkC"
      },
      "outputs": [],
      "source": [
        "def indexesFromWord(lang, word):\n",
        "    return [lang.token2index[token] for token in tokenizeWord(lang, word)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mB_R4Q4EZkC"
      },
      "source": [
        "## Get Tensor From Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEnFRlT3EZkC"
      },
      "outputs": [],
      "source": [
        "def tensorFromWord(lang, word):\n",
        "    indexes = indexesFromWord(lang, word)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETlZxqLWEZkQ"
      },
      "source": [
        "## Get Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n35BMmx_I-uW"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(input_lang, output_lang, pairs, batch_size, maxWordLength):\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, maxWordLength), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, maxWordLength), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromWord(input_lang, inp)\n",
        "        tgt_ids = indexesFromWord(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=2, persistent_workers=True)\n",
        "\n",
        "    return train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhXXvT8FEZkQ"
      },
      "source": [
        "## Train Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlunAKeKI-uW"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, scaler):\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad(set_to_none=True)\n",
        "        decoder_optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "          encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "          decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "          loss = criterion(\n",
        "              decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "              target_tensor.view(-1)\n",
        "          )\n",
        "        #autocast until here\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        scaler.step(encoder_optimizer)\n",
        "        scaler.step(decoder_optimizer)\n",
        "\n",
        "        scaler.update()\n",
        "#        loss.backward()\n",
        "\n",
        "#        encoder_optimizer.step()\n",
        "#        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Iej77AEZkQ"
      },
      "source": [
        "## Get Time as Minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOPmJeW4I-uX"
      },
      "outputs": [],
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-vMylRGEZkR"
      },
      "source": [
        "## Get Time Since"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNEhUwA1EZkR"
      },
      "outputs": [],
      "source": [
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-TVa4feZSH"
      },
      "source": [
        "## Romanize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uobNMtOSebUY"
      },
      "outputs": [],
      "source": [
        "def romanize(language, word):\n",
        "    if language == 'zh':\n",
        "        return pinyin_jyutping_sentence.pinyin(word)\n",
        "    elif language == 'ja':\n",
        "        kks = pykakasi.kakasi()\n",
        "        conversionResult = kks.convert(word)\n",
        "        res = ''\n",
        "        for item in conversionResult:\n",
        "            res += item['hepburn']\n",
        "        return res\n",
        "    elif language == 'ko':\n",
        "        r = Romanizer(word)\n",
        "        return r.romanize()\n",
        "    else:\n",
        "        return word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv2A7mZFI-uY"
      },
      "source": [
        "## Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxnLPGPfI-uY"
      },
      "outputs": [],
      "source": [
        "def evaluate(word):\n",
        "    encoder = LC['model']['encoder']\n",
        "    decoder = LC['model']['decoder']\n",
        "    input_lang = LC['model']['input_lang']\n",
        "    output_lang = LC['model']['output_lang']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromWord(input_lang, word)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_tokens = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                break\n",
        "            decoded_tokens.append(output_lang.index2token[idx.item()])\n",
        "    return ''.join(decoded_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELpr0qaYEZkR"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewUKa--sI-uX"
      },
      "outputs": [],
      "source": [
        "def test(epoch):\n",
        "    testing = LC['model']['testing']\n",
        "\n",
        "    LC['model']['encoder'].eval()\n",
        "    LC['model']['decoder'].eval()\n",
        "\n",
        "    totals = {\n",
        "        'total': 0\n",
        "    }\n",
        "    totalsarr = []\n",
        "    res = []\n",
        "\n",
        "    for i in range(0, len(testing[0])):\n",
        "        if testing[0][i] != 'total':\n",
        "            testlang = testing[0][i]\n",
        "            testword = testing[1][i]\n",
        "            expected = testing[2][i]\n",
        "\n",
        "            actual = evaluate(testword)\n",
        "            if testlang not in totals:\n",
        "              totals[testlang] = 0\n",
        "            if actual == expected:\n",
        "                res.append('')\n",
        "                totals[testlang] += 1\n",
        "                totals['total'] += 1\n",
        "            else:\n",
        "                res.append(\"%s\\n(%s)\" % (actual, romanize(LC['lang'], actual)))\n",
        "        else:\n",
        "            if testing[1][i] != 'all':\n",
        "                testlang = testing[1][i]\n",
        "                res.append(totals[testlang])\n",
        "                totalsarr.append(totals[testlang])\n",
        "                if testlang == 'lv':\n",
        "                    LC['model']['config']['testing_latest_lv'] = totals[testlang]\n",
        "\n",
        "                    if totals[testlang] > LC['model']['config']['testing_high_lv']:\n",
        "                        LC['model']['config']['testing_high_lv'] = totals[testlang]\n",
        "            else:\n",
        "                res.append(totals['total'])\n",
        "    res.append(epoch)\n",
        "\n",
        "    LC['model']['testing'].append(res)\n",
        "    median = statistics.median(totalsarr)\n",
        "    # Consider moving this elsewhere\n",
        "    LC['model']['config']['testing_latest_total'] = totals['total']\n",
        "    LC['model']['config']['testing_latest_median'] = median\n",
        "    LC['model']['config']['testing_latest_epoch'] = epoch\n",
        "    if (totals['total'] > LC['model']['config']['testing_high_total']) or (totals['total'] == LC['model']['config']['testing_high_total'] and median > LC['model']['config']['testing_high_median']):\n",
        "        LC['model']['config']['testing_high_total'] = totals['total']\n",
        "        LC['model']['config']['testing_high_median'] = median\n",
        "        LC['model']['config']['testing_high_epoch'] = epoch\n",
        "\n",
        "    # Return to traning mode\n",
        "    LC['model']['encoder'].train()\n",
        "    LC['model']['decoder'].train()\n",
        "\n",
        "    return totals['total'], median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soJt6XMtRALG"
      },
      "source": [
        "## Verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thTuw2G9RALI"
      },
      "outputs": [],
      "source": [
        "def verify(epoch):\n",
        "    testing = LC['model']['verify']\n",
        "\n",
        "    LC['model']['encoder'].eval()\n",
        "    LC['model']['decoder'].eval()\n",
        "\n",
        "    totals = {\n",
        "        'total': 0\n",
        "    }\n",
        "    totalsarr = []\n",
        "    res = []\n",
        "\n",
        "    for i in range(0, len(testing[0])):\n",
        "        if testing[0][i] != 'total':\n",
        "            testlang = testing[0][i]\n",
        "            testword = testing[1][i]\n",
        "            expected = testing[2][i]\n",
        "\n",
        "            actual = evaluate(testword)\n",
        "            if testlang not in totals:\n",
        "              totals[testlang] = 0\n",
        "            if actual == expected:\n",
        "                res.append('')\n",
        "                totals[testlang] += 1\n",
        "                totals['total'] += 1\n",
        "            else:\n",
        "                res.append(\"%s\\n(%s)\" % (actual, romanize(LC['lang'], actual)))\n",
        "        else:\n",
        "            if testing[1][i] != 'all':\n",
        "                testlang = testing[1][i]\n",
        "                res.append(totals[testlang])\n",
        "                totalsarr.append(totals[testlang])\n",
        "            else:\n",
        "                res.append(totals['total'])\n",
        "    res.append(epoch)\n",
        "\n",
        "    LC['model']['verify'].append(res)\n",
        "    median = statistics.median(totalsarr)\n",
        "    # Consider moving this elsewhere\n",
        "    LC['model']['config']['verify_latest_total'] = totals['total']\n",
        "    LC['model']['config']['verify_latest_median'] = median\n",
        "    LC['model']['config']['verify_latest_epoch'] = epoch\n",
        "    if (totals['total'] > LC['model']['config']['verify_high_total']) or (totals['total'] == LC['model']['config']['verify_high_total'] and median > LC['model']['config']['verify_high_median']):\n",
        "        LC['model']['config']['verify_high_total'] = totals['total']\n",
        "        LC['model']['config']['verify_high_median'] = median\n",
        "        LC['model']['config']['verify_high_epoch'] = epoch\n",
        "\n",
        "    # Return to traning mode\n",
        "    LC['model']['encoder'].train()\n",
        "    LC['model']['decoder'].train()\n",
        "\n",
        "    return totals['total'], median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEtguqQXIaZ2"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcHhtELmIaZ3"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, test_every=50, plot_every=100):\n",
        "    start = time.time()\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, scaler)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        LC['model']['config']['epoch_count'] += 1\n",
        "\n",
        "        if epoch % test_every == 0:\n",
        "            totalsV, medianV = verify(epoch)\n",
        "            totalsT, medianT = test(epoch)\n",
        "            if totalsT == LC['model']['config']['testing_high_total']:\n",
        "                isHighScore = 'HIGH SCORE!'\n",
        "            else:\n",
        "                isHighScore = ''\n",
        "            print(\"%s %s total score %s/%s (%s), median %s. verify score %s/%s (%s), median %s.\" % (isHighScore, epoch, totalsT, LC['model']['config']['testing_max_total'], str(round(totalsT / LC['model']['config']['testing_max_total'] * 100, 2)) + '%', medianT, totalsV, LC['model']['config']['verify_max_total'], str(round(totalsV / LC['model']['config']['verify_max_total'] * 100, 2)) + '%', medianV))\n",
        "            if (totalsT == LC['model']['config']['testing_high_total'] and LC['to_save'] and (totalsT > LC['model']['config']['testing_saved_total'] or LC['model']['config']['testing_latest_lv'] > LC['model']['config']['testing_saved_lv'] or LC['model']['config']['verify_latest_total'] > LC['model']['config']['verify_saved_total'])):\n",
        "                LC['model']['config']['last_trained_at'] = getDateTime()\n",
        "                saveModel()\n",
        "            if totalsT == LC['model']['config']['testing_max_total']:\n",
        "                LC['to_save'] = False\n",
        "                break\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            LC['model']['loss_plot'].append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.8f' % (timeSince(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLbS1kBoEZkT"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXJSyx49EZkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e22922-b03c-4426-83c8-6a9b046be363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  data/training-v12-full.zip\n",
            "  inflating: data/ja.tsv             \n",
            "  inflating: data/ko.tsv             \n",
            "  inflating: data/zh.tsv             \n",
            "Archive:  testing/testing-v12.zip\n",
            "  inflating: testing/testing_ja.json  \n",
            "  inflating: testing/testing_ko.json  \n",
            "  inflating: testing/testing_zh.json  \n",
            "  inflating: testing/verify_ja.json  \n",
            "  inflating: testing/verify_ko.json  \n",
            "  inflating: testing/verify_zh.json  \n",
            "ja-v12-1536-full (ja):\n",
            "*  Input tokens: 99\n",
            "*  Output tokens: 86\n",
            "*  Hidden size: 1536\n",
            "*  Max length: 17\n",
            "*  Epochs: 0\n",
            "*  Created at: 2024-05-05_16-28-32\n",
            "*  Last trained at: False\n",
            "*  Pairs: 183619\n"
          ]
        }
      ],
      "source": [
        "if not LC['is_initialized']:\n",
        "    %matplotlib inline\n",
        "\n",
        "    # Create directories\n",
        "    !mkdir -p \"{LC['data_root']}\" \"{LC['model_root']}\" \"{LC['testing_root']}\"\n",
        "\n",
        "    if LC['is_colab']:\n",
        "        from google.colab import drive\n",
        "        from google.colab import runtime\n",
        "        mountColab()\n",
        "\n",
        "    torch.multiprocessing.set_start_method('forkserver')\n",
        "\n",
        "    torch.autograd.set_detect_anomaly(False, check_nan=False)\n",
        "    torch.autograd.profiler.profile(enabled=False)\n",
        "    torch.autograd.profiler.emit_nvtx(enabled=False)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if LC['to_load']:\n",
        "        LC['model'] = loadModel()\n",
        "    else:\n",
        "        LC['model'] = createModel()\n",
        "\n",
        "    print(\"%s (%s):\" % (LC['model']['config']['title'], LC['model']['config']['lang']));\n",
        "    print(\"*  Input tokens: %s\" % LC['model']['input_lang'].n_tokens);\n",
        "    print(\"*  Output tokens: %s\" % LC['model']['output_lang'].n_tokens);\n",
        "    print(\"*  Hidden size: %s\" % LC['model']['config']['hidden_size']);\n",
        "    print(\"*  Max length: %s\" % LC['model']['config']['max_length']);\n",
        "    print(\"*  Epochs: %s\" % LC['model']['config']['epoch_count']);\n",
        "    print(\"*  Created at: %s\" % LC['model']['config']['created_at']);\n",
        "    print(\"*  Last trained at: %s\" % LC['model']['config']['last_trained_at']);\n",
        "    print(\"*  Pairs: %s\" % len(LC['model']['pairs']));\n",
        "\n",
        "    LC['is_initialized'] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdJtgce7I-uY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVbidY61I-uY",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff75c109-246a-4330-c9fa-be7a36e7ed99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs to train: 500\n",
            "HIGH SCORE! 5 total score 67/195 (34.36%), median 6. verify score 50/195 (25.64%), median 4.\n",
            "Saving...\n",
            "Saved!\n",
            "2m 25s (- 240m 52s) (5 1%) 0.40373224\n",
            "HIGH SCORE! 10 total score 96/195 (49.23%), median 8. verify score 95/195 (48.72%), median 7.\n",
            "Saving...\n",
            "Saved!\n",
            "4m 46s (- 234m 8s) (10 2%) 0.11596207\n",
            "HIGH SCORE! 15 total score 97/195 (49.74%), median 8. verify score 111/195 (56.92%), median 8.\n",
            "Saving...\n",
            "Saved!\n",
            "7m 8s (- 230m 42s) (15 3%) 0.06364830\n",
            " 20 total score 94/195 (48.21%), median 7. verify score 133/195 (68.21%), median 10.\n",
            "9m 15s (- 222m 17s) (20 4%) 0.03845032\n",
            " 25 total score 92/195 (47.18%), median 6. verify score 142/195 (72.82%), median 11.\n",
            "11m 24s (- 216m 38s) (25 5%) 0.02244005\n",
            " 30 total score 94/195 (48.21%), median 7. verify score 146/195 (74.87%), median 12.\n",
            "13m 32s (- 212m 12s) (30 6%) 0.01338041\n",
            " 35 total score 95/195 (48.72%), median 7. verify score 159/195 (81.54%), median 12.\n",
            "15m 40s (- 208m 11s) (35 7%) 0.00776343\n",
            "HIGH SCORE! 40 total score 99/195 (50.77%), median 7. verify score 163/195 (83.59%), median 13.\n",
            "Saving...\n",
            "Saved!\n",
            "18m 0s (- 207m 4s) (40 8%) 0.00437717\n",
            " 45 total score 94/195 (48.21%), median 7. verify score 153/195 (78.46%), median 13.\n",
            "20m 7s (- 203m 31s) (45 9%) 0.00496729\n",
            " 50 total score 91/195 (46.67%), median 6. verify score 161/195 (82.56%), median 12.\n",
            "22m 15s (- 200m 22s) (50 10%) 0.00818805\n",
            " 55 total score 97/195 (49.74%), median 8. verify score 169/195 (86.67%), median 14.\n",
            "24m 24s (- 197m 26s) (55 11%) 0.00190903\n",
            "HIGH SCORE! 60 total score 100/195 (51.28%), median 8. verify score 179/195 (91.79%), median 14.\n",
            "Saving...\n",
            "Saved!\n",
            "26m 44s (- 196m 8s) (60 12%) 0.00055816\n",
            "HIGH SCORE! 65 total score 101/195 (51.79%), median 8. verify score 178/195 (91.28%), median 14.\n",
            "Saving...\n",
            "Saved!\n",
            "29m 4s (- 194m 37s) (65 13%) 0.00034759\n",
            " 70 total score 97/195 (49.74%), median 8. verify score 175/195 (89.74%), median 14.\n",
            "31m 12s (- 191m 40s) (70 14%) 0.00027736\n",
            "HIGH SCORE! 75 total score 101/195 (51.79%), median 8. verify score 179/195 (91.79%), median 14.\n",
            "Saving...\n",
            "Saved!\n",
            "33m 31s (- 190m 0s) (75 15%) 0.00023230\n",
            " 80 total score 99/195 (50.77%), median 8. verify score 179/195 (91.79%), median 14.\n",
            "35m 39s (- 187m 9s) (80 16%) 0.00020169\n",
            " 85 total score 92/195 (47.18%), median 7. verify score 155/195 (79.49%), median 13.\n",
            "37m 47s (- 184m 28s) (85 17%) 0.00049597\n",
            " 90 total score 90/195 (46.15%), median 7. verify score 119/195 (61.03%), median 9.\n",
            "39m 55s (- 181m 52s) (90 18%) 0.15886749\n",
            " 95 total score 90/195 (46.15%), median 6. verify score 149/195 (76.41%), median 12.\n",
            "42m 3s (- 179m 16s) (95 19%) 0.02725474\n",
            " 100 total score 89/195 (45.64%), median 7. verify score 169/195 (86.67%), median 13.\n",
            "44m 10s (- 176m 40s) (100 20%) 0.01105258\n",
            " 105 total score 90/195 (46.15%), median 7. verify score 175/195 (89.74%), median 14.\n",
            "46m 18s (- 174m 13s) (105 21%) 0.00525766\n",
            " 110 total score 88/195 (45.13%), median 6. verify score 175/195 (89.74%), median 14.\n",
            "48m 26s (- 171m 44s) (110 22%) 0.00289811\n",
            " 115 total score 90/195 (46.15%), median 7. verify score 182/195 (93.33%), median 14.\n",
            "50m 33s (- 169m 16s) (115 23%) 0.00174584\n",
            " 120 total score 93/195 (47.69%), median 7. verify score 180/195 (92.31%), median 14.\n",
            "52m 40s (- 166m 49s) (120 24%) 0.00126667\n",
            " 125 total score 85/195 (43.59%), median 6. verify score 176/195 (90.26%), median 14.\n",
            "54m 49s (- 164m 28s) (125 25%) 0.00201727\n",
            " 130 total score 84/195 (43.08%), median 7. verify score 180/195 (92.31%), median 14.\n",
            "56m 56s (- 162m 4s) (130 26%) 0.00366332\n",
            " 135 total score 88/195 (45.13%), median 6. verify score 185/195 (94.87%), median 14.\n",
            "59m 3s (- 159m 41s) (135 27%) 0.00159106\n",
            " 140 total score 90/195 (46.15%), median 7. verify score 184/195 (94.36%), median 14.\n",
            "61m 11s (- 157m 19s) (140 28%) 0.00058211\n",
            " 145 total score 90/195 (46.15%), median 7. verify score 183/195 (93.85%), median 14.\n",
            "63m 18s (- 154m 59s) (145 28%) 0.00037893\n",
            " 150 total score 91/195 (46.67%), median 7. verify score 184/195 (94.36%), median 14.\n",
            "65m 26s (- 152m 41s) (150 30%) 0.00031271\n",
            " 155 total score 87/195 (44.62%), median 7. verify score 185/195 (94.87%), median 14.\n",
            "67m 33s (- 150m 22s) (155 31%) 0.00028782\n",
            " 160 total score 80/195 (41.03%), median 6. verify score 101/195 (51.79%), median 7.\n",
            "69m 41s (- 148m 4s) (160 32%) 0.01886544\n",
            " 165 total score 88/195 (45.13%), median 6. verify score 157/195 (80.51%), median 12.\n",
            "71m 49s (- 145m 48s) (165 33%) 0.03771367\n",
            " 170 total score 90/195 (46.15%), median 6. verify score 179/195 (91.79%), median 14.\n",
            "73m 57s (- 143m 33s) (170 34%) 0.00467834\n",
            " 175 total score 93/195 (47.69%), median 7. verify score 179/195 (91.79%), median 14.\n",
            "76m 4s (- 141m 16s) (175 35%) 0.00105313\n",
            " 180 total score 95/195 (48.72%), median 7. verify score 180/195 (92.31%), median 14.\n",
            "78m 11s (- 139m 0s) (180 36%) 0.00062148\n",
            " 185 total score 91/195 (46.67%), median 7. verify score 180/195 (92.31%), median 14.\n",
            "80m 19s (- 136m 45s) (185 37%) 0.00047051\n",
            " 190 total score 92/195 (47.18%), median 7. verify score 181/195 (92.82%), median 14.\n",
            "82m 26s (- 134m 29s) (190 38%) 0.00037162\n",
            " 195 total score 95/195 (48.72%), median 7. verify score 182/195 (93.33%), median 14.\n",
            "84m 32s (- 132m 14s) (195 39%) 0.00030719\n",
            " 200 total score 91/195 (46.67%), median 6. verify score 179/195 (91.79%), median 14.\n",
            "86m 39s (- 129m 59s) (200 40%) 0.00026263\n",
            " 205 total score 90/195 (46.15%), median 6. verify score 160/195 (82.05%), median 12.\n",
            "88m 47s (- 127m 46s) (205 41%) 0.00161088\n",
            " 210 total score 87/195 (44.62%), median 6. verify score 148/195 (75.9%), median 11.\n",
            "90m 55s (- 125m 33s) (210 42%) 0.04047802\n",
            " 215 total score 90/195 (46.15%), median 6. verify score 180/195 (92.31%), median 14.\n",
            "93m 2s (- 123m 20s) (215 43%) 0.00965827\n",
            " 220 total score 90/195 (46.15%), median 6. verify score 182/195 (93.33%), median 14.\n",
            "95m 9s (- 121m 6s) (220 44%) 0.00121520\n",
            " 225 total score 89/195 (45.64%), median 7. verify score 182/195 (93.33%), median 14.\n",
            "97m 17s (- 118m 54s) (225 45%) 0.00055918\n",
            " 230 total score 90/195 (46.15%), median 7. verify score 184/195 (94.36%), median 15.\n",
            "99m 24s (- 116m 42s) (230 46%) 0.00040137\n",
            " 235 total score 91/195 (46.67%), median 7. verify score 185/195 (94.87%), median 15.\n",
            "101m 31s (- 114m 29s) (235 47%) 0.00031713\n",
            " 240 total score 90/195 (46.15%), median 6. verify score 187/195 (95.9%), median 15.\n",
            "103m 38s (- 112m 16s) (240 48%) 0.00026715\n",
            " 245 total score 94/195 (48.21%), median 8. verify score 185/195 (94.87%), median 14.\n",
            "105m 45s (- 110m 4s) (245 49%) 0.00023134\n",
            " 250 total score 92/195 (47.18%), median 6. verify score 187/195 (95.9%), median 15.\n",
            "107m 53s (- 107m 53s) (250 50%) 0.00020500\n",
            " 255 total score 95/195 (48.72%), median 8. verify score 187/195 (95.9%), median 15.\n",
            "110m 0s (- 105m 41s) (255 51%) 0.00019095\n",
            " 260 total score 92/195 (47.18%), median 7. verify score 187/195 (95.9%), median 15.\n",
            "112m 7s (- 103m 29s) (260 52%) 0.00017253\n",
            " 265 total score 91/195 (46.67%), median 7. verify score 187/195 (95.9%), median 15.\n",
            "114m 13s (- 101m 17s) (265 53%) 0.00015956\n",
            " 270 total score 96/195 (49.23%), median 7. verify score 181/195 (92.82%), median 14.\n",
            "116m 20s (- 99m 6s) (270 54%) 0.00024401\n",
            " 275 total score 72/195 (36.92%), median 5. verify score 121/195 (62.05%), median 10.\n",
            "118m 29s (- 96m 56s) (275 55%) 0.05824769\n",
            " 280 total score 88/195 (45.13%), median 7. verify score 168/195 (86.15%), median 13.\n",
            "120m 36s (- 94m 45s) (280 56%) 0.01584078\n",
            " 285 total score 96/195 (49.23%), median 8. verify score 175/195 (89.74%), median 14.\n",
            "122m 42s (- 92m 34s) (285 56%) 0.00221308\n",
            " 290 total score 97/195 (49.74%), median 8. verify score 181/195 (92.82%), median 14.\n",
            "124m 49s (- 90m 23s) (290 57%) 0.00092035\n",
            " 295 total score 98/195 (50.26%), median 8. verify score 179/195 (91.79%), median 14.\n",
            "126m 56s (- 88m 12s) (295 59%) 0.00060473\n",
            " 300 total score 96/195 (49.23%), median 7. verify score 180/195 (92.31%), median 14.\n",
            "129m 3s (- 86m 2s) (300 60%) 0.00047463\n",
            " 305 total score 97/195 (49.74%), median 8. verify score 179/195 (91.79%), median 14.\n",
            "131m 11s (- 83m 52s) (305 61%) 0.00038313\n",
            " 310 total score 99/195 (50.77%), median 8. verify score 182/195 (93.33%), median 14.\n",
            "133m 19s (- 81m 42s) (310 62%) 0.00032165\n",
            " 315 total score 98/195 (50.26%), median 8. verify score 182/195 (93.33%), median 15.\n",
            "135m 25s (- 79m 32s) (315 63%) 0.00027340\n",
            "HIGH SCORE! 320 total score 102/195 (52.31%), median 8. verify score 183/195 (93.85%), median 14.\n",
            "Saving...\n",
            "Saved!\n",
            "137m 45s (- 77m 29s) (320 64%) 0.00024345\n",
            " 325 total score 100/195 (51.28%), median 7. verify score 184/195 (94.36%), median 14.\n",
            "139m 53s (- 75m 19s) (325 65%) 0.00022338\n",
            " 330 total score 99/195 (50.77%), median 7. verify score 185/195 (94.87%), median 15.\n",
            "142m 0s (- 73m 9s) (330 66%) 0.00020373\n",
            " 335 total score 60/195 (30.77%), median 4. verify score 101/195 (51.79%), median 8.\n",
            "144m 7s (- 70m 59s) (335 67%) 0.02172809\n",
            " 340 total score 78/195 (40.0%), median 6. verify score 144/195 (73.85%), median 11.\n",
            "146m 14s (- 68m 49s) (340 68%) 0.03591833\n",
            " 345 total score 88/195 (45.13%), median 7. verify score 172/195 (88.21%), median 14.\n",
            "148m 22s (- 66m 39s) (345 69%) 0.00482925\n",
            " 350 total score 89/195 (45.64%), median 6. verify score 180/195 (92.31%), median 14.\n",
            "150m 30s (- 64m 30s) (350 70%) 0.00112362\n",
            " 355 total score 93/195 (47.69%), median 7. verify score 181/195 (92.82%), median 14.\n",
            "152m 36s (- 62m 20s) (355 71%) 0.00066169\n",
            " 360 total score 93/195 (47.69%), median 7. verify score 180/195 (92.31%), median 14.\n",
            "154m 43s (- 60m 10s) (360 72%) 0.00047814\n",
            " 365 total score 96/195 (49.23%), median 7. verify score 179/195 (91.79%), median 14.\n",
            "156m 50s (- 58m 0s) (365 73%) 0.00038880\n",
            " 370 total score 100/195 (51.28%), median 7. verify score 178/195 (91.28%), median 14.\n",
            "158m 57s (- 55m 50s) (370 74%) 0.00035705\n",
            " 375 total score 99/195 (50.77%), median 7. verify score 184/195 (94.36%), median 14.\n",
            "161m 4s (- 53m 41s) (375 75%) 0.00029723\n",
            " 380 total score 101/195 (51.79%), median 7. verify score 183/195 (93.85%), median 15.\n",
            "163m 11s (- 51m 32s) (380 76%) 0.00025088\n",
            " 385 total score 98/195 (50.26%), median 7. verify score 183/195 (93.85%), median 14.\n",
            "165m 18s (- 49m 22s) (385 77%) 0.00021971\n",
            " 390 total score 101/195 (51.79%), median 7. verify score 182/195 (93.33%), median 14.\n",
            "167m 24s (- 47m 13s) (390 78%) 0.00020248\n",
            " 395 total score 98/195 (50.26%), median 7. verify score 186/195 (95.38%), median 15.\n",
            "169m 31s (- 45m 3s) (395 79%) 0.00018594\n",
            " 400 total score 97/195 (49.74%), median 7. verify score 184/195 (94.36%), median 14.\n",
            "171m 38s (- 42m 54s) (400 80%) 0.00018049\n",
            " 405 total score 94/195 (48.21%), median 7. verify score 181/195 (92.82%), median 14.\n",
            "173m 45s (- 40m 45s) (405 81%) 0.00021467\n",
            " 410 total score 63/195 (32.31%), median 4. verify score 112/195 (57.44%), median 9.\n",
            "175m 53s (- 38m 36s) (410 82%) 0.04742407\n",
            " 415 total score 87/195 (44.62%), median 7. verify score 160/195 (82.05%), median 12.\n",
            "178m 0s (- 36m 27s) (415 83%) 0.02089816\n",
            " 420 total score 91/195 (46.67%), median 8. verify score 167/195 (85.64%), median 13.\n",
            "180m 7s (- 34m 18s) (420 84%) 0.00312067\n",
            " 425 total score 92/195 (47.18%), median 8. verify score 168/195 (86.15%), median 13.\n",
            "182m 14s (- 32m 9s) (425 85%) 0.00109162\n",
            " 430 total score 91/195 (46.67%), median 7. verify score 171/195 (87.69%), median 13.\n",
            "184m 20s (- 30m 0s) (430 86%) 0.00069029\n",
            " 435 total score 92/195 (47.18%), median 7. verify score 176/195 (90.26%), median 14.\n",
            "186m 27s (- 27m 51s) (435 87%) 0.00052082\n",
            " 440 total score 97/195 (49.74%), median 8. verify score 175/195 (89.74%), median 14.\n",
            "188m 35s (- 25m 42s) (440 88%) 0.00041696\n",
            " 445 total score 96/195 (49.23%), median 8. verify score 171/195 (87.69%), median 14.\n",
            "190m 42s (- 23m 34s) (445 89%) 0.00035085\n",
            " 450 total score 96/195 (49.23%), median 8. verify score 173/195 (88.72%), median 14.\n",
            "192m 49s (- 21m 25s) (450 90%) 0.00031372\n",
            " 455 total score 94/195 (48.21%), median 7. verify score 177/195 (90.77%), median 14.\n",
            "194m 56s (- 19m 16s) (455 91%) 0.00029043\n",
            " 460 total score 94/195 (48.21%), median 8. verify score 173/195 (88.72%), median 14.\n",
            "197m 2s (- 17m 8s) (460 92%) 0.00026072\n",
            " 465 total score 95/195 (48.72%), median 7. verify score 170/195 (87.18%), median 13.\n",
            "199m 9s (- 14m 59s) (465 93%) 0.00023431\n",
            " 470 total score 96/195 (49.23%), median 7. verify score 172/195 (88.21%), median 13.\n",
            "201m 17s (- 12m 50s) (470 94%) 0.00026608\n",
            " 475 total score 62/195 (31.79%), median 5. verify score 101/195 (51.79%), median 8.\n",
            "203m 24s (- 10m 42s) (475 95%) 0.01899995\n",
            " 480 total score 75/195 (38.46%), median 6. verify score 140/195 (71.79%), median 12.\n",
            "205m 31s (- 8m 33s) (480 96%) 0.03360377\n",
            " 485 total score 80/195 (41.03%), median 6. verify score 159/195 (81.54%), median 13.\n",
            "207m 37s (- 6m 25s) (485 97%) 0.00543202\n",
            " 490 total score 78/195 (40.0%), median 6. verify score 165/195 (84.62%), median 13.\n",
            "209m 44s (- 4m 16s) (490 98%) 0.00135955\n",
            " 495 total score 82/195 (42.05%), median 6. verify score 170/195 (87.18%), median 14.\n",
            "211m 50s (- 2m 8s) (495 99%) 0.00075499\n",
            " 500 total score 82/195 (42.05%), median 6. verify score 172/195 (88.21%), median 14.\n",
            "213m 57s (- 0m 0s) (500 100%) 0.00054817\n"
          ]
        }
      ],
      "source": [
        "if LC['to_train']:\n",
        "    LC['model']['encoder'].train()\n",
        "    LC['model']['decoder'].train()\n",
        "    LC['model']['config']['last_trained_at'] = getDateTime()\n",
        "\n",
        "    train_dataloader = get_dataloader(\n",
        "        LC['model']['input_lang'],\n",
        "        LC['model']['output_lang'],\n",
        "        LC['model']['pairs'],\n",
        "        LC['batch_size'],\n",
        "        LC['model']['config']['max_length']\n",
        "    )\n",
        "\n",
        "    for inputs, targets in train_dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    epoch_count = LC['epoch_goal'] - LC['model']['config']['epoch_count']\n",
        "    print(\"Epochs to train: %s\" % epoch_count)\n",
        "    train(\n",
        "        train_dataloader,\n",
        "        LC['model']['encoder'],\n",
        "        LC['model']['decoder'],\n",
        "        epoch_count,\n",
        "        # print_every=math.floor(epoch_count/100*1),\n",
        "        print_every=5,\n",
        "        test_every=5,\n",
        "        # learning_rate=0.00075,\n",
        "        plot_every=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsKbeX2jI-ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f75877-fcdb-48e3-d489-63806f5c1a51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n",
            "Saved!\n"
          ]
        }
      ],
      "source": [
        "if LC['to_save']:\n",
        "    if LC['model']['config']['testing_max_total'] != LC['model']['config']['testing_latest_total']:\n",
        "      LC['model_path'] = os.path.join(LC['model_root'], LC['model']['config']['lang'], LC['model']['config']['title'] + '-' + str(LC['model']['config']['epoch_count']))\n",
        "      LC['model_export_path'] = os.path.join(LC['drive_root'], LC['model_root'], LC['model']['config']['lang'], LC['model']['config']['title'] + '-' + str(LC['model']['config']['epoch_count']) + '.zip')\n",
        "      saveModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv3LF9kCEZkV"
      },
      "source": [
        "# Exit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd4iSHnR0N6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d303c5-fccc-4698-e7fd-806f8ee1e2f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  5 08:03:03 PM UTC 2024\n"
          ]
        }
      ],
      "source": [
        "!date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBilzGreJJh9"
      },
      "outputs": [],
      "source": [
        "if LC['is_colab'] and LC['colab_auto_quit']:\n",
        "  runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "x2R2Za-nEZj8"
      ],
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}