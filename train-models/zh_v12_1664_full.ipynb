{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEq02TlSEZj4"
      },
      "source": [
        "# Main Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLYYoHNOEZj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a37aa74-8cbc-4717-e004-40617dec7b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipatok\n",
            "  Downloading ipatok-0.4.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting pykakasi\n",
            "  Downloading pykakasi-2.2.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pinyin_jyutping_sentence\n",
            "  Downloading pinyin_jyutping_sentence-1.3.tar.gz (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting korean_romanizer\n",
            "  Downloading korean_romanizer-0.25.1-py3-none-any.whl (18 kB)\n",
            "Collecting jaconv (from pykakasi)\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from pykakasi)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from pinyin_jyutping_sentence) (0.42.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pykakasi) (1.14.1)\n",
            "Building wheels for collected packages: pinyin_jyutping_sentence, jaconv\n",
            "  Building wheel for pinyin_jyutping_sentence (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin_jyutping_sentence: filename=pinyin_jyutping_sentence-1.3-py3-none-any.whl size=12492668 sha256=df29c8eda3d836032d6170ea569bdb1cda460b82206e56157b570090cbc4006a\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/b7/09/99fe76af71b53ba2d93146f4aba014579bd7d08ec8b3ad8754\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16416 sha256=ce66e72b3059c9308478f169f8615a8bf0b5d6b94dd06b0ff0b8bfa3b0c97ad4\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/8f/2e/a730bf1fca05b33e532d5d91dabdf406c9b718ec85b01b1b54\n",
            "Successfully built pinyin_jyutping_sentence jaconv\n",
            "Installing collected packages: korean_romanizer, jaconv, ipatok, pinyin_jyutping_sentence, deprecated, pykakasi\n",
            "Successfully installed deprecated-1.2.14 ipatok-0.4.2 jaconv-0.3.4 korean_romanizer-0.25.1 pinyin_jyutping_sentence-1.3 pykakasi-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install ipatok pykakasi pinyin_jyutping_sentence korean_romanizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVH-kSeSEZj6"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "from ipatok import tokenise\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import shutil\n",
        "import copy\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import pinyin_jyutping_sentence\n",
        "import pykakasi\n",
        "from korean_romanizer.romanizer import Romanizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "import numbers\n",
        "import csv\n",
        "\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmR1Jln5EZj7"
      },
      "source": [
        "# Local Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jydTPps2EZj7"
      },
      "outputs": [],
      "source": [
        "LC = {\n",
        "    'is_initialized': False,\n",
        "    'is_colab': True if os.getenv(\"COLAB_RELEASE_TAG\") else False,\n",
        "    'colab_auto_quit': True,\n",
        "    'to_train': True,\n",
        "    'lang': 'zh',\n",
        "    'to_load': False,\n",
        "    'to_load_model': '2024-04-20_21-50-55',\n",
        "    'to_save': True,\n",
        "    'drive_mount': '/content/drive',\n",
        "    'drive_root': '/content/drive/My Drive/seq2seq/',\n",
        "    'src_data': 'training-v12-full.zip',\n",
        "    'src_testing': 'testing-v12.zip',\n",
        "    'batch_size': int(3900 * (1536 / (1664 + 40))), # can be 'auto' or a number\n",
        "    'epoch_goal': 500,\n",
        "    'data_root': 'data',\n",
        "    'testing_root': 'testing',\n",
        "    'model_root': 'models',\n",
        "    'model_path': '',\n",
        "    'model_export_path': '',\n",
        "    'default_model_config': {\n",
        "        'lang': False,\n",
        "        'hidden_size': 1664,\n",
        "        'max_length': 17,\n",
        "        'epoch_count': 0,\n",
        "        'last_trained_at': False,\n",
        "        'created_at': False,\n",
        "        'title': 'zh-v12-1664-full',\n",
        "        'testing_high_total': 0,\n",
        "        'testing_high_lv': 0,\n",
        "        'testing_high_median': 0,\n",
        "        'testing_high_epoch': 0,\n",
        "        'testing_saved_total': 0,\n",
        "        'testing_saved_lv': 0,\n",
        "        'testing_latest_total': 0,\n",
        "        'testing_latest_lv': 0,\n",
        "        'testing_latest_median': 0,\n",
        "        'testing_latest_epoch': 0,\n",
        "        'testing_max_total': 0,\n",
        "        'verify_high_total': 0,\n",
        "        'verify_high_median': 0,\n",
        "        'verify_high_epoch': 0,\n",
        "        'verify_saved_total': 0,\n",
        "        'verify_latest_total': 0,\n",
        "        'verify_latest_median': 0,\n",
        "        'verify_latest_epoch': 0,\n",
        "        'verify_max_total': 0,\n",
        "    },\n",
        "    'model': False\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2R2Za-nEZj8"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED6aEkUOEZj8"
      },
      "source": [
        "## Lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl417qlnEZj8"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, isIpa = False):\n",
        "        self.token2index = {}\n",
        "        self.token2count = {}\n",
        "        self.index2token = {0: \"?\", 1: \"EOS\" }\n",
        "        self.n_tokens = len(self.index2token.keys())\n",
        "        self.isIpa = isIpa\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if self.isIpa:\n",
        "            word, language = splitInput(word)\n",
        "            if language:\n",
        "                self.addToken(language)\n",
        "\n",
        "            word = standardiseIpa(word)\n",
        "\n",
        "        for token in word:\n",
        "            self.addToken(token)\n",
        "\n",
        "    def addToken(self, token):\n",
        "        if token not in self.token2index:\n",
        "            self.token2index[token] = self.n_tokens\n",
        "            self.token2count[token] = 1\n",
        "            self.index2token[self.n_tokens] = token\n",
        "            self.n_tokens += 1\n",
        "        else:\n",
        "            self.token2count[token] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrNAe3J2I-uU"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLUasu-4I-uU"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlwr36wEEZj8"
      },
      "source": [
        "## Attention (Bahdanau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM83aftSEZj9"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzF7cX93I-uU"
      },
      "source": [
        "## Decoder (with attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjNm1sakI-uV"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, maxWordLength, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.maxWordLength = maxWordLength\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(self.maxWordLength):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp5A1n0HEZj9"
      },
      "source": [
        "# Initialization Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGp620D4EZj9"
      },
      "source": [
        "## Get Date Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6SI2tkkEZj-"
      },
      "outputs": [],
      "source": [
        "def getDateTime():\n",
        "    return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QvlgChJEZj-"
      },
      "source": [
        "## Mount COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWO_LZiHEZj-"
      },
      "outputs": [],
      "source": [
        "def mountColab():\n",
        "  drive.mount(LC['drive_mount'])\n",
        "\n",
        "  !cp \"{os.path.join(LC['drive_root'], LC['src_data'])}\" \"{os.path.join(LC['data_root'], LC['src_data'])}\"\n",
        "  !cp \"{os.path.join(LC['drive_root'], LC['src_testing'])}\" \"{os.path.join(LC['testing_root'], LC['src_testing'])}\"\n",
        "\n",
        "  !unzip -o \"{os.path.join(LC['data_root'], LC['src_data'])}\" -d \"{os.path.join(LC['data_root'])}\"\n",
        "  !unzip -o \"{os.path.join(LC['testing_root'], LC['src_testing'])}\" -d \"{os.path.join(LC['testing_root'])}\"\n",
        "  !rm \"{os.path.join(LC['data_root'], LC['src_data'])}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOyd8QaKEZj-"
      },
      "source": [
        "## Get Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DV6mCeNI-uS"
      },
      "outputs": [],
      "source": [
        "def getPairs(filename):\n",
        "    # Read the file and split into lines\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs\n",
        "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVdF5sisIaZs"
      },
      "source": [
        "## Get Testing File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XejdeiGgIaZt"
      },
      "outputs": [],
      "source": [
        "def getTestingFile(filename):\n",
        "    # Read the file and split into lines\n",
        "    content = open(filename, encoding='utf-8').read().strip()\n",
        "\n",
        "    return json.loads(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfsuXKo0EZj-"
      },
      "source": [
        "## Create Lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAk2ISpqEZj-"
      },
      "outputs": [],
      "source": [
        "def createLang(pairs, pairIndex, isIpa):\n",
        "    lang = Lang(isIpa)\n",
        "\n",
        "    for pair in pairs:\n",
        "        lang.addWord(pair[pairIndex])\n",
        "\n",
        "    return lang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkZZfrc6EZj_"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lMEO4XuEZj_"
      },
      "outputs": [],
      "source": [
        "def createModel():\n",
        "    config = copy.copy(LC['default_model_config'])\n",
        "    config['created_at'] = getDateTime()\n",
        "    config['lang'] = LC['lang']\n",
        "\n",
        "    LC['model_path'] = os.path.join(LC['model_root'], config['lang'], config['title'])\n",
        "    LC['model_export_path'] = os.path.join(LC['drive_root'], LC['model_root'], config['lang'], config['title'] + '.zip')\n",
        "\n",
        "    pairs = getPairs(os.path.join(LC['data_root'], config['lang'] + '.tsv'))\n",
        "\n",
        "    if LC['batch_size'] == 'auto':\n",
        "        LC['batch_size'] = len(pairs)\n",
        "\n",
        "    inputLang = createLang(pairs, 0, True)\n",
        "    outputLang = createLang(pairs, 1, False)\n",
        "\n",
        "    encoder = EncoderRNN(inputLang.n_tokens, config['hidden_size']).to(device)\n",
        "    decoder = AttnDecoderRNN(config['hidden_size'], outputLang.n_tokens, config['max_length']).to(device)\n",
        "    encoder = encoder.to(memory_format=torch.channels_last)\n",
        "    decoder = decoder.to(memory_format=torch.channels_last)\n",
        "\n",
        "    testing = getTestingFile(os.path.join(LC['testing_root'], 'testing_%s.json' % config['lang']))\n",
        "    config['testing_max_total'] = testing[2][len(testing[2]) - 1]\n",
        "    verifyData = getTestingFile(os.path.join(LC['testing_root'], 'verify_%s.json' % config['lang']))\n",
        "    config['verify_max_total'] = verifyData[2][len(verifyData[2]) - 1]\n",
        "\n",
        "    romanized = []\n",
        "\n",
        "    for item in testing[2]:\n",
        "      if not isinstance(item, numbers.Number):\n",
        "        romanized.append(romanize(config['lang'], item))\n",
        "\n",
        "    testing.append(romanized)\n",
        "\n",
        "    return {\n",
        "        'config': config,\n",
        "        'input_lang': inputLang,\n",
        "        'output_lang': outputLang,\n",
        "        'pairs': pairs,\n",
        "        'encoder': encoder,\n",
        "        'decoder': decoder,\n",
        "        'loss_plot': [],\n",
        "        'testing': testing,\n",
        "        'verify': verifyData\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em237Hc6EZj_"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiBv2dhIEZj_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def loadModel():\n",
        "    LC['model_export_path'] = os.path.join(LC['drive_root'], LC['model_root'], LC['lang'], LC['to_load_model'] + '.zip')\n",
        "    LC['model_path'] = os.path.join(LC['model_root'], LC['lang'], LC['to_load_model'])\n",
        "\n",
        "    !mkdir -p \"{LC['model_path']}\"\n",
        "    if LC['is_colab']:\n",
        "        !unzip -o \"{LC['model_export_path']}\" -d \"{LC['model_path']}\"\n",
        "\n",
        "    config = False\n",
        "    with open(os.path.join(LC['model_path'], 'config.json'), 'r') as f:\n",
        "        config = json.load(f)\n",
        "    LC['lang'] = config['lang']\n",
        "\n",
        "    loss_plot = False\n",
        "    with open(os.path.join(LC['model_path'], 'loss_plot.json'), 'r') as f:\n",
        "        loss_plot = json.load(f)\n",
        "\n",
        "    pairs = getPairs(os.path.join(LC['model_path'], 'pairs.tsv'))\n",
        "\n",
        "    inputLang = False\n",
        "    with open(os.path.join(LC['model_path'], 'input_lang.pickle'), 'rb') as f:\n",
        "        inputLang = pickle.load(f)\n",
        "\n",
        "    outputLang = False\n",
        "    with open(os.path.join(LC['model_path'], 'output_lang.pickle'), 'rb') as f:\n",
        "        outputLang = pickle.load(f)\n",
        "\n",
        "    encoder = EncoderRNN(inputLang.n_tokens, config['hidden_size']).to(device)\n",
        "    decoder = AttnDecoderRNN(config['hidden_size'], outputLang.n_tokens, config['max_length']).to(device)\n",
        "\n",
        "    encoder.load_state_dict(torch.load(os.path.join(LC['model_path'], 'encoder.pth')))\n",
        "    decoder.load_state_dict(torch.load(os.path.join(LC['model_path'], 'decoder.pth')))\n",
        "    encoder = encoder.to(memory_format=torch.channels_last)\n",
        "    decoder = decoder.to(memory_format=torch.channels_last)\n",
        "\n",
        "    testing = False\n",
        "    with open(os.path.join(LC['model_path'], 'testing.json')) as f:\n",
        "        testing = json.load(f)\n",
        "\n",
        "    LC['model']['config']['testing_max_total'] = testing[2][len(testing[2]) - 1]\n",
        "\n",
        "    verifyData = False\n",
        "    with open(os.path.join(LC['model_path'], 'verify.json')) as f:\n",
        "        verifyData = json.load(f)\n",
        "\n",
        "    LC['model']['config']['verify_max_total'] = verifyData[2][len(verifyData[2]) - 1]\n",
        "\n",
        "    return {\n",
        "        'config': config,\n",
        "        'input_lang': inputLang,\n",
        "        'output_lang': outputLang,\n",
        "        'pairs': pairs,\n",
        "        'encoder': encoder,\n",
        "        'decoder': decoder,\n",
        "        'loss_plot': loss_plot,\n",
        "        'testing': testing,\n",
        "        'verify': verifyData\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E73D6DAtEZj_"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz_A1RJlEZkA"
      },
      "outputs": [],
      "source": [
        "def saveModel():\n",
        "    print('Saving...')\n",
        "    !mkdir -p \"{LC['model_path']}\"\n",
        "\n",
        "    LC['model']['config']['testing_saved_total'] = LC['model']['config']['testing_latest_total']\n",
        "    LC['model']['config']['testing_saved_lv'] = LC['model']['config']['testing_latest_lv']\n",
        "    LC['model']['config']['verify_saved_total'] = LC['model']['config']['verify_latest_total']\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'config.json'), 'w') as f:\n",
        "        json.dump(LC['model']['config'], f)\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'loss_plot.json'), 'w') as f:\n",
        "        json.dump(LC['model']['loss_plot'], f)\n",
        "\n",
        "    lines = []\n",
        "    for pair in LC['model']['pairs']:\n",
        "        lines.append(\"\\t\".join(pair))\n",
        "    with open(os.path.join(LC['model_path'], 'pairs.tsv'), 'w') as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'input_lang.pickle'), 'wb') as f:\n",
        "        pickle.dump(LC['model']['input_lang'], f)\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'output_lang.pickle'), 'wb') as f:\n",
        "        pickle.dump(LC['model']['output_lang'], f)\n",
        "\n",
        "    torch.save(LC['model']['encoder'].state_dict(), os.path.join(LC['model_path'], 'encoder.pth'))\n",
        "    torch.save(LC['model']['decoder'].state_dict(), os.path.join(LC['model_path'], 'decoder.pth'))\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'testing.json'), 'w') as f:\n",
        "        json.dump(LC['model']['testing'], f)\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'verify.json'), 'w') as f:\n",
        "        json.dump(LC['model']['verify'], f)\n",
        "\n",
        "    df = pd.DataFrame(LC['model']['testing'])\n",
        "    df.to_csv(os.path.join(LC['model_path'], 'testing.csv'), quoting=csv.QUOTE_NONNUMERIC)\n",
        "\n",
        "    df = pd.DataFrame(LC['model']['verify'])\n",
        "    df.to_csv(os.path.join(LC['model_path'], 'verify.csv'), quoting=csv.QUOTE_NONNUMERIC)\n",
        "\n",
        "    if LC['is_colab']:\n",
        "        !zip -q \"{LC['model_export_path']}\" -j \"{os.path.join('.', LC['model_path'])}\"/*\n",
        "\n",
        "    print('Saved!')\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkjmrV7aEZkA"
      },
      "source": [
        "## Standardise IPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGxerzZwEZkA"
      },
      "outputs": [],
      "source": [
        "def standardiseIpa(word):\n",
        "    return ''.join(tokenise(word, strict=False, replace=True, diphthongs=False, tones=False, unknown=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QooT_7bGEZkA"
      },
      "source": [
        "## Split Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itBY8ib9EZkA"
      },
      "outputs": [],
      "source": [
        "def splitInput(inputWord):\n",
        "    word = ''\n",
        "    language = False\n",
        "    if len(inputWord) > 2:\n",
        "        if inputWord[2] == \"_\":\n",
        "            language = inputWord[:3]\n",
        "            word = inputWord[3:]\n",
        "        else:\n",
        "            word = inputWord\n",
        "    else:\n",
        "        word = inputWord\n",
        "\n",
        "    return word, language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXauv9YtEZkB"
      },
      "source": [
        "## Tokenize Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM9zBBJlEZkB"
      },
      "outputs": [],
      "source": [
        "def tokenizeWord(lang, word):\n",
        "    tokens = []\n",
        "    if lang.isIpa:\n",
        "      word, language = splitInput(word)\n",
        "\n",
        "      if language:\n",
        "          tokens.append(language)\n",
        "\n",
        "      word = ''.join(tokenise(word, strict=False, replace=True, diphthongs=False, tones=False, unknown=False))\n",
        "    for token in word:\n",
        "        tokens.append(token)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QbGT71EZkB"
      },
      "source": [
        "## Get Indexes From Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqLiypl0EZkC"
      },
      "outputs": [],
      "source": [
        "def indexesFromWord(lang, word):\n",
        "    return [lang.token2index[token] for token in tokenizeWord(lang, word)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mB_R4Q4EZkC"
      },
      "source": [
        "## Get Tensor From Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEnFRlT3EZkC"
      },
      "outputs": [],
      "source": [
        "def tensorFromWord(lang, word):\n",
        "    indexes = indexesFromWord(lang, word)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETlZxqLWEZkQ"
      },
      "source": [
        "## Get Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n35BMmx_I-uW"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(input_lang, output_lang, pairs, batch_size, maxWordLength):\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, maxWordLength), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, maxWordLength), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromWord(input_lang, inp)\n",
        "        tgt_ids = indexesFromWord(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=2, persistent_workers=True)\n",
        "\n",
        "    return train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhXXvT8FEZkQ"
      },
      "source": [
        "## Train Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlunAKeKI-uW"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, scaler):\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad(set_to_none=True)\n",
        "        decoder_optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "          encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "          decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "          loss = criterion(\n",
        "              decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "              target_tensor.view(-1)\n",
        "          )\n",
        "        #autocast until here\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        scaler.step(encoder_optimizer)\n",
        "        scaler.step(decoder_optimizer)\n",
        "\n",
        "        scaler.update()\n",
        "#        loss.backward()\n",
        "\n",
        "#        encoder_optimizer.step()\n",
        "#        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Iej77AEZkQ"
      },
      "source": [
        "## Get Time as Minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOPmJeW4I-uX"
      },
      "outputs": [],
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-vMylRGEZkR"
      },
      "source": [
        "## Get Time Since"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNEhUwA1EZkR"
      },
      "outputs": [],
      "source": [
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-TVa4feZSH"
      },
      "source": [
        "## Romanize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uobNMtOSebUY"
      },
      "outputs": [],
      "source": [
        "def romanize(language, word):\n",
        "    if language == 'zh':\n",
        "        return pinyin_jyutping_sentence.pinyin(word)\n",
        "    elif language == 'ja':\n",
        "        kks = pykakasi.kakasi()\n",
        "        conversionResult = kks.convert(word)\n",
        "        res = ''\n",
        "        for item in conversionResult:\n",
        "            res += item['hepburn']\n",
        "        return res\n",
        "    elif language == 'ko':\n",
        "        r = Romanizer(word)\n",
        "        return r.romanize()\n",
        "    else:\n",
        "        return word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv2A7mZFI-uY"
      },
      "source": [
        "## Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxnLPGPfI-uY"
      },
      "outputs": [],
      "source": [
        "def evaluate(word):\n",
        "    encoder = LC['model']['encoder']\n",
        "    decoder = LC['model']['decoder']\n",
        "    input_lang = LC['model']['input_lang']\n",
        "    output_lang = LC['model']['output_lang']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromWord(input_lang, word)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_tokens = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                break\n",
        "            decoded_tokens.append(output_lang.index2token[idx.item()])\n",
        "    return ''.join(decoded_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELpr0qaYEZkR"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewUKa--sI-uX"
      },
      "outputs": [],
      "source": [
        "def test(epoch):\n",
        "    testing = LC['model']['testing']\n",
        "\n",
        "    LC['model']['encoder'].eval()\n",
        "    LC['model']['decoder'].eval()\n",
        "\n",
        "    totals = {\n",
        "        'total': 0\n",
        "    }\n",
        "    totalsarr = []\n",
        "    res = []\n",
        "\n",
        "    for i in range(0, len(testing[0])):\n",
        "        if testing[0][i] != 'total':\n",
        "            testlang = testing[0][i]\n",
        "            testword = testing[1][i]\n",
        "            expected = testing[2][i]\n",
        "\n",
        "            actual = evaluate(testword)\n",
        "            if testlang not in totals:\n",
        "              totals[testlang] = 0\n",
        "            if actual == expected:\n",
        "                res.append('')\n",
        "                totals[testlang] += 1\n",
        "                totals['total'] += 1\n",
        "            else:\n",
        "                res.append(\"%s\\n(%s)\" % (actual, romanize(LC['lang'], actual)))\n",
        "        else:\n",
        "            if testing[1][i] != 'all':\n",
        "                testlang = testing[1][i]\n",
        "                res.append(totals[testlang])\n",
        "                totalsarr.append(totals[testlang])\n",
        "                if testlang == 'lv':\n",
        "                    LC['model']['config']['testing_latest_lv'] = totals[testlang]\n",
        "\n",
        "                    if totals[testlang] > LC['model']['config']['testing_high_lv']:\n",
        "                        LC['model']['config']['testing_high_lv'] = totals[testlang]\n",
        "            else:\n",
        "                res.append(totals['total'])\n",
        "    res.append(epoch)\n",
        "\n",
        "    LC['model']['testing'].append(res)\n",
        "    median = statistics.median(totalsarr)\n",
        "    # Consider moving this elsewhere\n",
        "    LC['model']['config']['testing_latest_total'] = totals['total']\n",
        "    LC['model']['config']['testing_latest_median'] = median\n",
        "    LC['model']['config']['testing_latest_epoch'] = epoch\n",
        "    if (totals['total'] > LC['model']['config']['testing_high_total']) or (totals['total'] == LC['model']['config']['testing_high_total'] and median > LC['model']['config']['testing_high_median']):\n",
        "        LC['model']['config']['testing_high_total'] = totals['total']\n",
        "        LC['model']['config']['testing_high_median'] = median\n",
        "        LC['model']['config']['testing_high_epoch'] = epoch\n",
        "\n",
        "    # Return to traning mode\n",
        "    LC['model']['encoder'].train()\n",
        "    LC['model']['decoder'].train()\n",
        "\n",
        "    return totals['total'], median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soJt6XMtRALG"
      },
      "source": [
        "## Verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thTuw2G9RALI"
      },
      "outputs": [],
      "source": [
        "def verify(epoch):\n",
        "    testing = LC['model']['verify']\n",
        "\n",
        "    LC['model']['encoder'].eval()\n",
        "    LC['model']['decoder'].eval()\n",
        "\n",
        "    totals = {\n",
        "        'total': 0\n",
        "    }\n",
        "    totalsarr = []\n",
        "    res = []\n",
        "\n",
        "    for i in range(0, len(testing[0])):\n",
        "        if testing[0][i] != 'total':\n",
        "            testlang = testing[0][i]\n",
        "            testword = testing[1][i]\n",
        "            expected = testing[2][i]\n",
        "\n",
        "            actual = evaluate(testword)\n",
        "            if testlang not in totals:\n",
        "              totals[testlang] = 0\n",
        "            if actual == expected:\n",
        "                res.append('')\n",
        "                totals[testlang] += 1\n",
        "                totals['total'] += 1\n",
        "            else:\n",
        "                res.append(\"%s\\n(%s)\" % (actual, romanize(LC['lang'], actual)))\n",
        "        else:\n",
        "            if testing[1][i] != 'all':\n",
        "                testlang = testing[1][i]\n",
        "                res.append(totals[testlang])\n",
        "                totalsarr.append(totals[testlang])\n",
        "            else:\n",
        "                res.append(totals['total'])\n",
        "    res.append(epoch)\n",
        "\n",
        "    LC['model']['verify'].append(res)\n",
        "    median = statistics.median(totalsarr)\n",
        "    # Consider moving this elsewhere\n",
        "    LC['model']['config']['verify_latest_total'] = totals['total']\n",
        "    LC['model']['config']['verify_latest_median'] = median\n",
        "    LC['model']['config']['verify_latest_epoch'] = epoch\n",
        "    if (totals['total'] > LC['model']['config']['verify_high_total']) or (totals['total'] == LC['model']['config']['verify_high_total'] and median > LC['model']['config']['verify_high_median']):\n",
        "        LC['model']['config']['verify_high_total'] = totals['total']\n",
        "        LC['model']['config']['verify_high_median'] = median\n",
        "        LC['model']['config']['verify_high_epoch'] = epoch\n",
        "\n",
        "    # Return to traning mode\n",
        "    LC['model']['encoder'].train()\n",
        "    LC['model']['decoder'].train()\n",
        "\n",
        "    return totals['total'], median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEtguqQXIaZ2"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcHhtELmIaZ3"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, test_every=50, plot_every=100):\n",
        "    start = time.time()\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, scaler)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        LC['model']['config']['epoch_count'] += 1\n",
        "\n",
        "        if epoch % test_every == 0:\n",
        "            totalsV, medianV = verify(epoch)\n",
        "            totalsT, medianT = test(epoch)\n",
        "            if totalsT == LC['model']['config']['testing_high_total']:\n",
        "                isHighScore = 'HIGH SCORE!'\n",
        "            else:\n",
        "                isHighScore = ''\n",
        "            print(\"%s %s total score %s/%s (%s), median %s. verify score %s/%s (%s), median %s.\" % (isHighScore, epoch, totalsT, LC['model']['config']['testing_max_total'], str(round(totalsT / LC['model']['config']['testing_max_total'] * 100, 2)) + '%', medianT, totalsV, LC['model']['config']['verify_max_total'], str(round(totalsV / LC['model']['config']['verify_max_total'] * 100, 2)) + '%', medianV))\n",
        "            if (totalsT == LC['model']['config']['testing_high_total'] and LC['to_save'] and (totalsT > LC['model']['config']['testing_saved_total'] or LC['model']['config']['testing_latest_lv'] > LC['model']['config']['testing_saved_lv'] or LC['model']['config']['verify_latest_total'] > LC['model']['config']['verify_saved_total'])):\n",
        "                LC['model']['config']['last_trained_at'] = getDateTime()\n",
        "                saveModel()\n",
        "            if totalsT == LC['model']['config']['testing_max_total']:\n",
        "                LC['to_save'] = False\n",
        "                break\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            LC['model']['loss_plot'].append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.8f' % (timeSince(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLbS1kBoEZkT"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXJSyx49EZkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc083ee6-c186-4e25-9621-941735c5092c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  data/training-v12-full.zip\n",
            "  inflating: data/ja.tsv             \n",
            "  inflating: data/ko.tsv             \n",
            "  inflating: data/zh.tsv             \n",
            "Archive:  testing/testing-v12.zip\n",
            "  inflating: testing/testing_ja.json  \n",
            "  inflating: testing/testing_ko.json  \n",
            "  inflating: testing/testing_zh.json  \n",
            "  inflating: testing/verify_ja.json  \n",
            "  inflating: testing/verify_ko.json  \n",
            "  inflating: testing/verify_zh.json  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.697 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.697 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zh-v12-1664-full (zh):\n",
            "*  Input tokens: 99\n",
            "*  Output tokens: 3048\n",
            "*  Hidden size: 1664\n",
            "*  Max length: 17\n",
            "*  Epochs: 0\n",
            "*  Created at: 2024-05-05_22-16-23\n",
            "*  Last trained at: False\n",
            "*  Pairs: 143273\n"
          ]
        }
      ],
      "source": [
        "if not LC['is_initialized']:\n",
        "    %matplotlib inline\n",
        "\n",
        "    # Create directories\n",
        "    !mkdir -p \"{LC['data_root']}\" \"{LC['model_root']}\" \"{LC['testing_root']}\"\n",
        "\n",
        "    if LC['is_colab']:\n",
        "        from google.colab import drive\n",
        "        from google.colab import runtime\n",
        "        mountColab()\n",
        "\n",
        "    torch.multiprocessing.set_start_method('forkserver')\n",
        "\n",
        "    torch.autograd.set_detect_anomaly(False, check_nan=False)\n",
        "    torch.autograd.profiler.profile(enabled=False)\n",
        "    torch.autograd.profiler.emit_nvtx(enabled=False)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if LC['to_load']:\n",
        "        LC['model'] = loadModel()\n",
        "    else:\n",
        "        LC['model'] = createModel()\n",
        "\n",
        "    print(\"%s (%s):\" % (LC['model']['config']['title'], LC['model']['config']['lang']));\n",
        "    print(\"*  Input tokens: %s\" % LC['model']['input_lang'].n_tokens);\n",
        "    print(\"*  Output tokens: %s\" % LC['model']['output_lang'].n_tokens);\n",
        "    print(\"*  Hidden size: %s\" % LC['model']['config']['hidden_size']);\n",
        "    print(\"*  Max length: %s\" % LC['model']['config']['max_length']);\n",
        "    print(\"*  Epochs: %s\" % LC['model']['config']['epoch_count']);\n",
        "    print(\"*  Created at: %s\" % LC['model']['config']['created_at']);\n",
        "    print(\"*  Last trained at: %s\" % LC['model']['config']['last_trained_at']);\n",
        "    print(\"*  Pairs: %s\" % len(LC['model']['pairs']));\n",
        "\n",
        "    LC['is_initialized'] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdJtgce7I-uY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVbidY61I-uY",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f24dbe-b6a0-4795-f5bf-69895eda924c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs to train: 500\n",
            "HIGH SCORE! 5 total score 60/195 (30.77%), median 5. verify score 68/195 (34.87%), median 5.\n",
            "Saving...\n",
            "Saved!\n",
            "2m 22s (- 234m 31s) (5 1%) 0.57504971\n",
            "HIGH SCORE! 10 total score 73/195 (37.44%), median 6. verify score 92/195 (47.18%), median 7.\n",
            "Saving...\n",
            "Saved!\n",
            "4m 40s (- 228m 52s) (10 2%) 0.15643610\n",
            " 15 total score 68/195 (34.87%), median 5. verify score 108/195 (55.38%), median 8.\n",
            "6m 41s (- 216m 10s) (15 3%) 0.08977518\n",
            " 20 total score 64/195 (32.82%), median 5. verify score 120/195 (61.54%), median 10.\n",
            "8m 43s (- 209m 15s) (20 4%) 0.05306205\n",
            "HIGH SCORE! 25 total score 73/195 (37.44%), median 4. verify score 141/195 (72.31%), median 11.\n",
            "Saving...\n",
            "Saved!\n",
            "11m 1s (- 209m 23s) (25 5%) 0.03317980\n",
            " 30 total score 70/195 (35.9%), median 5. verify score 146/195 (74.87%), median 12.\n",
            "13m 2s (- 204m 16s) (30 6%) 0.02284380\n",
            "HIGH SCORE! 35 total score 73/195 (37.44%), median 6. verify score 141/195 (72.31%), median 12.\n",
            "Saving...\n",
            "Saved!\n",
            "15m 20s (- 203m 44s) (35 7%) 0.01632280\n",
            "HIGH SCORE! 40 total score 74/195 (37.95%), median 5. verify score 152/195 (77.95%), median 12.\n",
            "Saving...\n",
            "Saved!\n",
            "17m 37s (- 202m 37s) (40 8%) 0.01277230\n",
            " 45 total score 62/195 (31.79%), median 5. verify score 147/195 (75.38%), median 11.\n",
            "19m 38s (- 198m 32s) (45 9%) 0.01571805\n",
            " 50 total score 62/195 (31.79%), median 4. verify score 155/195 (79.49%), median 12.\n",
            "21m 39s (- 194m 52s) (50 10%) 0.01344694\n",
            "HIGH SCORE! 55 total score 75/195 (38.46%), median 6. verify score 157/195 (80.51%), median 12.\n",
            "Saving...\n",
            "Saved!\n",
            "23m 57s (- 193m 46s) (55 11%) 0.00999178\n",
            " 60 total score 72/195 (36.92%), median 6. verify score 165/195 (84.62%), median 13.\n",
            "25m 58s (- 190m 26s) (60 12%) 0.00917262\n",
            " 65 total score 65/195 (33.33%), median 4. verify score 154/195 (78.97%), median 12.\n",
            "27m 58s (- 187m 14s) (65 13%) 0.00956541\n",
            " 70 total score 57/195 (29.23%), median 4. verify score 134/195 (68.72%), median 11.\n",
            "29m 59s (- 184m 16s) (70 14%) 0.02336802\n",
            " 75 total score 62/195 (31.79%), median 4. verify score 159/195 (81.54%), median 13.\n",
            "32m 0s (- 181m 24s) (75 15%) 0.01623168\n",
            " 80 total score 69/195 (35.38%), median 5. verify score 163/195 (83.59%), median 13.\n",
            "34m 1s (- 178m 39s) (80 16%) 0.00941480\n",
            " 85 total score 62/195 (31.79%), median 4. verify score 167/195 (85.64%), median 13.\n",
            "36m 3s (- 176m 2s) (85 17%) 0.00879118\n",
            " 90 total score 69/195 (35.38%), median 5. verify score 171/195 (87.69%), median 14.\n",
            "38m 4s (- 173m 28s) (90 18%) 0.00868842\n",
            " 95 total score 65/195 (33.33%), median 4. verify score 167/195 (85.64%), median 13.\n",
            "40m 6s (- 170m 58s) (95 19%) 0.00871925\n",
            " 100 total score 59/195 (30.26%), median 4. verify score 154/195 (78.97%), median 12.\n",
            "42m 9s (- 168m 36s) (100 20%) 0.00932386\n",
            " 105 total score 57/195 (29.23%), median 4. verify score 139/195 (71.28%), median 10.\n",
            "44m 11s (- 166m 14s) (105 21%) 0.03076973\n",
            " 110 total score 58/195 (29.74%), median 4. verify score 161/195 (82.56%), median 13.\n",
            "46m 13s (- 163m 54s) (110 22%) 0.01840648\n",
            " 115 total score 67/195 (34.36%), median 6. verify score 162/195 (83.08%), median 13.\n",
            "48m 15s (- 161m 32s) (115 23%) 0.00959005\n",
            " 120 total score 68/195 (34.87%), median 5. verify score 163/195 (83.59%), median 13.\n",
            "50m 16s (- 159m 11s) (120 24%) 0.00873119\n",
            " 125 total score 72/195 (36.92%), median 6. verify score 166/195 (85.13%), median 13.\n",
            "52m 17s (- 156m 51s) (125 25%) 0.00863234\n",
            " 130 total score 66/195 (33.85%), median 4. verify score 166/195 (85.13%), median 13.\n",
            "54m 17s (- 154m 31s) (130 26%) 0.00864770\n",
            " 135 total score 66/195 (33.85%), median 5. verify score 172/195 (88.21%), median 14.\n",
            "56m 18s (- 152m 14s) (135 27%) 0.00865731\n",
            " 140 total score 61/195 (31.28%), median 4. verify score 164/195 (84.1%), median 13.\n",
            "58m 19s (- 149m 58s) (140 28%) 0.00907691\n",
            " 145 total score 46/195 (23.59%), median 3. verify score 122/195 (62.56%), median 8.\n",
            "60m 20s (- 147m 44s) (145 28%) 0.02150132\n",
            " 150 total score 54/195 (27.69%), median 4. verify score 150/195 (76.92%), median 12.\n",
            "62m 22s (- 145m 31s) (150 30%) 0.02953492\n",
            " 155 total score 64/195 (32.82%), median 4. verify score 174/195 (89.23%), median 13.\n",
            "64m 23s (- 143m 18s) (155 31%) 0.01087868\n",
            " 160 total score 70/195 (35.9%), median 6. verify score 177/195 (90.77%), median 13.\n",
            "66m 24s (- 141m 6s) (160 32%) 0.00874071\n",
            " 165 total score 63/195 (32.31%), median 5. verify score 178/195 (91.28%), median 14.\n",
            "68m 25s (- 138m 56s) (165 33%) 0.00840900\n",
            " 170 total score 69/195 (35.38%), median 5. verify score 177/195 (90.77%), median 14.\n",
            "70m 27s (- 136m 46s) (170 34%) 0.00838105\n",
            " 175 total score 67/195 (34.36%), median 4. verify score 181/195 (92.82%), median 14.\n",
            "72m 30s (- 134m 38s) (175 35%) 0.00843982\n",
            " 180 total score 65/195 (33.33%), median 4. verify score 175/195 (89.74%), median 14.\n",
            "74m 31s (- 132m 29s) (180 36%) 0.00845110\n",
            " 185 total score 64/195 (32.82%), median 4. verify score 176/195 (90.26%), median 14.\n",
            "76m 33s (- 130m 21s) (185 37%) 0.00859176\n",
            " 190 total score 61/195 (31.28%), median 4. verify score 162/195 (83.08%), median 13.\n",
            "78m 34s (- 128m 12s) (190 38%) 0.01043029\n",
            " 195 total score 57/195 (29.23%), median 4. verify score 133/195 (68.21%), median 10.\n",
            "80m 37s (- 126m 5s) (195 39%) 0.03770451\n",
            " 200 total score 65/195 (33.33%), median 5. verify score 159/195 (81.54%), median 12.\n",
            "82m 39s (- 123m 59s) (200 40%) 0.01633608\n",
            " 205 total score 64/195 (32.82%), median 4. verify score 163/195 (83.59%), median 13.\n",
            "84m 40s (- 121m 50s) (205 41%) 0.00912854\n",
            " 210 total score 63/195 (32.31%), median 4. verify score 170/195 (87.18%), median 13.\n",
            "86m 41s (- 119m 42s) (210 42%) 0.00838721\n",
            " 215 total score 61/195 (31.28%), median 4. verify score 171/195 (87.69%), median 14.\n",
            "88m 41s (- 117m 34s) (215 43%) 0.00825269\n",
            " 220 total score 67/195 (34.36%), median 4. verify score 176/195 (90.26%), median 14.\n",
            "90m 42s (- 115m 27s) (220 44%) 0.00825213\n",
            " 225 total score 64/195 (32.82%), median 5. verify score 177/195 (90.77%), median 14.\n",
            "92m 43s (- 113m 20s) (225 45%) 0.00829583\n",
            " 230 total score 67/195 (34.36%), median 5. verify score 173/195 (88.72%), median 14.\n",
            "94m 44s (- 111m 13s) (230 46%) 0.00843825\n",
            " 235 total score 67/195 (34.36%), median 5. verify score 172/195 (88.21%), median 14.\n",
            "96m 46s (- 109m 7s) (235 47%) 0.00852436\n",
            " 240 total score 58/195 (29.74%), median 4. verify score 167/195 (85.64%), median 13.\n",
            "98m 48s (- 107m 2s) (240 48%) 0.00911598\n",
            " 245 total score 41/195 (21.03%), median 3. verify score 115/195 (58.97%), median 9.\n",
            "100m 49s (- 104m 56s) (245 49%) 0.02661715\n",
            " 250 total score 51/195 (26.15%), median 4. verify score 150/195 (76.92%), median 12.\n",
            "102m 51s (- 102m 51s) (250 50%) 0.02478416\n",
            " 255 total score 59/195 (30.26%), median 4. verify score 155/195 (79.49%), median 12.\n",
            "104m 52s (- 100m 46s) (255 51%) 0.01014665\n",
            " 260 total score 53/195 (27.18%), median 4. verify score 162/195 (83.08%), median 12.\n",
            "106m 54s (- 98m 40s) (260 52%) 0.00850736\n",
            " 265 total score 56/195 (28.72%), median 4. verify score 163/195 (83.59%), median 13.\n",
            "108m 55s (- 96m 35s) (265 53%) 0.00820412\n",
            " 270 total score 58/195 (29.74%), median 4. verify score 165/195 (84.62%), median 13.\n",
            "110m 57s (- 94m 30s) (270 54%) 0.00813241\n",
            " 275 total score 57/195 (29.23%), median 4. verify score 170/195 (87.18%), median 14.\n",
            "112m 58s (- 92m 25s) (275 55%) 0.00808924\n",
            " 280 total score 61/195 (31.28%), median 4. verify score 167/195 (85.64%), median 13.\n",
            "114m 59s (- 90m 21s) (280 56%) 0.00820135\n",
            " 285 total score 65/195 (33.33%), median 5. verify score 166/195 (85.13%), median 13.\n",
            "117m 0s (- 88m 16s) (285 56%) 0.00830885\n",
            " 290 total score 60/195 (30.77%), median 4. verify score 160/195 (82.05%), median 12.\n",
            "119m 2s (- 86m 12s) (290 57%) 0.00851349\n",
            " 295 total score 59/195 (30.26%), median 4. verify score 153/195 (78.46%), median 12.\n",
            "121m 3s (- 84m 7s) (295 59%) 0.00928185\n",
            " 300 total score 48/195 (24.62%), median 4. verify score 118/195 (60.51%), median 9.\n",
            "123m 5s (- 82m 3s) (300 60%) 0.02926025\n",
            " 305 total score 52/195 (26.67%), median 4. verify score 145/195 (74.36%), median 11.\n",
            "125m 7s (- 79m 59s) (305 61%) 0.02113228\n",
            " 310 total score 55/195 (28.21%), median 4. verify score 164/195 (84.1%), median 12.\n",
            "127m 8s (- 77m 55s) (310 62%) 0.00981164\n",
            " 315 total score 57/195 (29.23%), median 3. verify score 168/195 (86.15%), median 13.\n",
            "129m 9s (- 75m 51s) (315 63%) 0.00836214\n",
            " 320 total score 55/195 (28.21%), median 4. verify score 170/195 (87.18%), median 14.\n",
            "131m 11s (- 73m 47s) (320 64%) 0.00813585\n",
            " 325 total score 53/195 (27.18%), median 4. verify score 163/195 (83.59%), median 12.\n",
            "133m 12s (- 71m 43s) (325 65%) 0.00809985\n",
            " 330 total score 55/195 (28.21%), median 4. verify score 167/195 (85.64%), median 13.\n",
            "135m 13s (- 69m 39s) (330 66%) 0.00811546\n",
            " 335 total score 58/195 (29.74%), median 4. verify score 170/195 (87.18%), median 13.\n",
            "137m 14s (- 67m 35s) (335 67%) 0.00814040\n",
            " 340 total score 56/195 (28.72%), median 4. verify score 170/195 (87.18%), median 13.\n",
            "139m 15s (- 65m 31s) (340 68%) 0.00825392\n",
            " 345 total score 55/195 (28.21%), median 4. verify score 165/195 (84.62%), median 13.\n",
            "141m 16s (- 63m 28s) (345 69%) 0.00844162\n",
            " 350 total score 50/195 (25.64%), median 5. verify score 162/195 (83.08%), median 13.\n",
            "143m 17s (- 61m 24s) (350 70%) 0.00897785\n",
            " 355 total score 48/195 (24.62%), median 4. verify score 108/195 (55.38%), median 9.\n",
            "145m 18s (- 59m 21s) (355 71%) 0.01981990\n",
            " 360 total score 50/195 (25.64%), median 4. verify score 140/195 (71.79%), median 11.\n",
            "147m 19s (- 57m 17s) (360 72%) 0.02721730\n",
            " 365 total score 55/195 (28.21%), median 4. verify score 148/195 (75.9%), median 11.\n",
            "149m 20s (- 55m 14s) (365 73%) 0.01132664\n",
            " 370 total score 53/195 (27.18%), median 4. verify score 154/195 (78.97%), median 12.\n",
            "151m 21s (- 53m 10s) (370 74%) 0.00870027\n",
            " 375 total score 52/195 (26.67%), median 4. verify score 155/195 (79.49%), median 12.\n",
            "153m 22s (- 51m 7s) (375 75%) 0.00817646\n",
            " 380 total score 52/195 (26.67%), median 4. verify score 155/195 (79.49%), median 12.\n",
            "155m 23s (- 49m 4s) (380 76%) 0.00803960\n",
            " 385 total score 46/195 (23.59%), median 4. verify score 153/195 (78.46%), median 11.\n",
            "157m 24s (- 47m 0s) (385 77%) 0.00803874\n",
            " 390 total score 50/195 (25.64%), median 3. verify score 151/195 (77.44%), median 12.\n",
            "159m 25s (- 44m 57s) (390 78%) 0.00813792\n",
            " 395 total score 47/195 (24.1%), median 4. verify score 161/195 (82.56%), median 12.\n",
            "161m 26s (- 42m 54s) (395 79%) 0.00823567\n",
            " 400 total score 54/195 (27.69%), median 4. verify score 161/195 (82.56%), median 12.\n",
            "163m 27s (- 40m 51s) (400 80%) 0.00825463\n",
            " 405 total score 50/195 (25.64%), median 4. verify score 159/195 (81.54%), median 12.\n",
            "165m 28s (- 38m 48s) (405 81%) 0.00859593\n",
            " 410 total score 45/195 (23.08%), median 4. verify score 144/195 (73.85%), median 11.\n",
            "167m 29s (- 36m 45s) (410 82%) 0.01026138\n",
            " 415 total score 46/195 (23.59%), median 4. verify score 128/195 (65.64%), median 10.\n",
            "169m 30s (- 34m 43s) (415 83%) 0.02808299\n",
            " 420 total score 45/195 (23.08%), median 3. verify score 148/195 (75.9%), median 11.\n",
            "171m 31s (- 32m 40s) (420 84%) 0.01694639\n",
            " 425 total score 47/195 (24.1%), median 4. verify score 155/195 (79.49%), median 12.\n",
            "173m 32s (- 30m 37s) (425 85%) 0.00970363\n",
            " 430 total score 50/195 (25.64%), median 4. verify score 166/195 (85.13%), median 13.\n",
            "175m 33s (- 28m 34s) (430 86%) 0.00838671\n",
            " 435 total score 51/195 (26.15%), median 4. verify score 158/195 (81.03%), median 12.\n",
            "177m 34s (- 26m 32s) (435 87%) 0.00810275\n",
            " 440 total score 48/195 (24.62%), median 4. verify score 155/195 (79.49%), median 12.\n",
            "179m 36s (- 24m 29s) (440 88%) 0.00800478\n",
            " 445 total score 51/195 (26.15%), median 4. verify score 155/195 (79.49%), median 12.\n",
            "181m 37s (- 22m 26s) (445 89%) 0.00805853\n",
            " 450 total score 46/195 (23.59%), median 3. verify score 156/195 (80.0%), median 12.\n",
            "183m 38s (- 20m 24s) (450 90%) 0.00809069\n",
            " 455 total score 44/195 (22.56%), median 4. verify score 161/195 (82.56%), median 13.\n",
            "185m 39s (- 18m 21s) (455 91%) 0.00823135\n",
            " 460 total score 46/195 (23.59%), median 3. verify score 157/195 (80.51%), median 12.\n",
            "187m 40s (- 16m 19s) (460 92%) 0.00826752\n",
            " 465 total score 48/195 (24.62%), median 4. verify score 152/195 (77.95%), median 12.\n",
            "189m 41s (- 14m 16s) (465 93%) 0.00885341\n",
            " 470 total score 47/195 (24.1%), median 4. verify score 121/195 (62.05%), median 9.\n",
            "191m 42s (- 12m 14s) (470 94%) 0.01571398\n",
            " 475 total score 45/195 (23.08%), median 4. verify score 122/195 (62.56%), median 10.\n",
            "193m 42s (- 10m 11s) (475 95%) 0.02501608\n",
            " 480 total score 52/195 (26.67%), median 4. verify score 145/195 (74.36%), median 11.\n",
            "195m 44s (- 8m 9s) (480 96%) 0.01236627\n",
            " 485 total score 54/195 (27.69%), median 4. verify score 157/195 (80.51%), median 12.\n",
            "197m 45s (- 6m 6s) (485 97%) 0.00898115\n",
            " 490 total score 52/195 (26.67%), median 4. verify score 156/195 (80.0%), median 13.\n",
            "199m 47s (- 4m 4s) (490 98%) 0.00837835\n",
            " 495 total score 52/195 (26.67%), median 4. verify score 163/195 (83.59%), median 13.\n",
            "201m 48s (- 2m 2s) (495 99%) 0.00811133\n",
            " 500 total score 53/195 (27.18%), median 4. verify score 156/195 (80.0%), median 12.\n",
            "203m 50s (- 0m 0s) (500 100%) 0.00808708\n"
          ]
        }
      ],
      "source": [
        "if LC['to_train']:\n",
        "    LC['model']['encoder'].train()\n",
        "    LC['model']['decoder'].train()\n",
        "    LC['model']['config']['last_trained_at'] = getDateTime()\n",
        "\n",
        "    train_dataloader = get_dataloader(\n",
        "        LC['model']['input_lang'],\n",
        "        LC['model']['output_lang'],\n",
        "        LC['model']['pairs'],\n",
        "        LC['batch_size'],\n",
        "        LC['model']['config']['max_length']\n",
        "    )\n",
        "\n",
        "    for inputs, targets in train_dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    epoch_count = LC['epoch_goal'] - LC['model']['config']['epoch_count']\n",
        "    print(\"Epochs to train: %s\" % epoch_count)\n",
        "    train(\n",
        "        train_dataloader,\n",
        "        LC['model']['encoder'],\n",
        "        LC['model']['decoder'],\n",
        "        epoch_count,\n",
        "        # print_every=math.floor(epoch_count/100*1),\n",
        "        print_every=5,\n",
        "        test_every=5,\n",
        "        # learning_rate=0.00075,\n",
        "        plot_every=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsKbeX2jI-ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc412093-3409-48fa-9e95-776de766b320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        }
      ],
      "source": [
        "if LC['to_save']:\n",
        "    if LC['model']['config']['testing_max_total'] != LC['model']['config']['testing_latest_total']:\n",
        "      LC['model_path'] = os.path.join(LC['model_root'], LC['model']['config']['lang'], LC['model']['config']['title'] + '-' + str(LC['model']['config']['epoch_count']))\n",
        "      LC['model_export_path'] = os.path.join(LC['drive_root'], LC['model_root'], LC['model']['config']['lang'], LC['model']['config']['title'] + '-' + str(LC['model']['config']['epoch_count']) + '.zip')\n",
        "      saveModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv3LF9kCEZkV"
      },
      "source": [
        "# Exit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd4iSHnR0N6l"
      },
      "outputs": [],
      "source": [
        "!date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBilzGreJJh9"
      },
      "outputs": [],
      "source": [
        "if LC['is_colab'] and LC['colab_auto_quit']:\n",
        "  runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "x2R2Za-nEZj8"
      ],
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}