{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEq02TlSEZj4"
      },
      "source": [
        "# Main Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLYYoHNOEZj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0696dc-9f56-407b-8707-28b8df242866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipatok\n",
            "  Downloading ipatok-0.4.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting pykakasi\n",
            "  Downloading pykakasi-2.2.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pinyin_jyutping_sentence\n",
            "  Downloading pinyin_jyutping_sentence-1.3.tar.gz (12.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting korean_romanizer\n",
            "  Downloading korean_romanizer-0.25.1-py3-none-any.whl (18 kB)\n",
            "Collecting jaconv (from pykakasi)\n",
            "  Downloading jaconv-0.3.4.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated (from pykakasi)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from pinyin_jyutping_sentence) (0.42.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pykakasi) (1.14.1)\n",
            "Building wheels for collected packages: pinyin_jyutping_sentence, jaconv\n",
            "  Building wheel for pinyin_jyutping_sentence (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin_jyutping_sentence: filename=pinyin_jyutping_sentence-1.3-py3-none-any.whl size=12492668 sha256=196c1b670ce44eb219b4d04beb07e3466302cfdebc77404045f06119c1dacab4\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/b7/09/99fe76af71b53ba2d93146f4aba014579bd7d08ec8b3ad8754\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.3.4-py3-none-any.whl size=16416 sha256=9442838ab9412ae9708147fba1acc794e044b9fcb5f17a041091866ef1ce874a\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/8f/2e/a730bf1fca05b33e532d5d91dabdf406c9b718ec85b01b1b54\n",
            "Successfully built pinyin_jyutping_sentence jaconv\n",
            "Installing collected packages: korean_romanizer, jaconv, ipatok, pinyin_jyutping_sentence, deprecated, pykakasi\n",
            "Successfully installed deprecated-1.2.14 ipatok-0.4.2 jaconv-0.3.4 korean_romanizer-0.25.1 pinyin_jyutping_sentence-1.3 pykakasi-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install ipatok pykakasi pinyin_jyutping_sentence korean_romanizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVH-kSeSEZj6"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "from ipatok import tokenise\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import shutil\n",
        "import copy\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import pinyin_jyutping_sentence\n",
        "import pykakasi\n",
        "from korean_romanizer.romanizer import Romanizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "import numbers\n",
        "import csv\n",
        "\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmR1Jln5EZj7"
      },
      "source": [
        "# Local Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jydTPps2EZj7"
      },
      "outputs": [],
      "source": [
        "LC = {\n",
        "    'is_initialized': False,\n",
        "    'is_colab': True if os.getenv(\"COLAB_RELEASE_TAG\") else False,\n",
        "    'colab_auto_quit': True,\n",
        "    'to_train': True,\n",
        "    'lang': 'ko',\n",
        "    'to_load': False,\n",
        "    'to_load_model': '2024-04-20_21-50-55',\n",
        "    'to_save': True,\n",
        "    'drive_mount': '/content/drive',\n",
        "    'drive_root': '/content/drive/My Drive/seq2seq/',\n",
        "    'src_data': 'training-v12-full.zip',\n",
        "    'src_testing': 'testing-v12.zip',\n",
        "    'batch_size': 4800, # can be 'auto' or a number\n",
        "    'epoch_goal': 500,\n",
        "    'data_root': 'data',\n",
        "    'testing_root': 'testing',\n",
        "    'model_root': 'models',\n",
        "    'model_path': '',\n",
        "    'model_export_path': '',\n",
        "    'default_model_config': {\n",
        "        'lang': False,\n",
        "        'hidden_size': 1280,\n",
        "        'max_length': 17,\n",
        "        'epoch_count': 0,\n",
        "        'last_trained_at': False,\n",
        "        'created_at': False,\n",
        "        'title': 'ko-v12-1280-full',\n",
        "        'testing_high_total': 0,\n",
        "        'testing_high_lv': 0,\n",
        "        'testing_high_median': 0,\n",
        "        'testing_high_epoch': 0,\n",
        "        'testing_saved_total': 0,\n",
        "        'testing_saved_lv': 0,\n",
        "        'testing_latest_total': 0,\n",
        "        'testing_latest_lv': 0,\n",
        "        'testing_latest_median': 0,\n",
        "        'testing_latest_epoch': 0,\n",
        "        'testing_max_total': 0,\n",
        "        'verify_high_total': 0,\n",
        "        'verify_high_median': 0,\n",
        "        'verify_high_epoch': 0,\n",
        "        'verify_saved_total': 0,\n",
        "        'verify_latest_total': 0,\n",
        "        'verify_latest_median': 0,\n",
        "        'verify_latest_epoch': 0,\n",
        "        'verify_max_total': 0,\n",
        "    },\n",
        "    'model': False\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2R2Za-nEZj8"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED6aEkUOEZj8"
      },
      "source": [
        "## Lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl417qlnEZj8"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, isIpa = False):\n",
        "        self.token2index = {}\n",
        "        self.token2count = {}\n",
        "        self.index2token = {0: \"?\", 1: \"EOS\" }\n",
        "        self.n_tokens = len(self.index2token.keys())\n",
        "        self.isIpa = isIpa\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if self.isIpa:\n",
        "            word, language = splitInput(word)\n",
        "            if language:\n",
        "                self.addToken(language)\n",
        "\n",
        "            word = standardiseIpa(word)\n",
        "\n",
        "        for token in word:\n",
        "            self.addToken(token)\n",
        "\n",
        "    def addToken(self, token):\n",
        "        if token not in self.token2index:\n",
        "            self.token2index[token] = self.n_tokens\n",
        "            self.token2count[token] = 1\n",
        "            self.index2token[self.n_tokens] = token\n",
        "            self.n_tokens += 1\n",
        "        else:\n",
        "            self.token2count[token] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrNAe3J2I-uU"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLUasu-4I-uU"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlwr36wEEZj8"
      },
      "source": [
        "## Attention (Bahdanau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM83aftSEZj9"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzF7cX93I-uU"
      },
      "source": [
        "## Decoder (with attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjNm1sakI-uV"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, maxWordLength, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.maxWordLength = maxWordLength\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(self.maxWordLength):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp5A1n0HEZj9"
      },
      "source": [
        "# Initialization Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGp620D4EZj9"
      },
      "source": [
        "## Get Date Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6SI2tkkEZj-"
      },
      "outputs": [],
      "source": [
        "def getDateTime():\n",
        "    return datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QvlgChJEZj-"
      },
      "source": [
        "## Mount COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWO_LZiHEZj-"
      },
      "outputs": [],
      "source": [
        "def mountColab():\n",
        "  drive.mount(LC['drive_mount'])\n",
        "\n",
        "  !cp \"{os.path.join(LC['drive_root'], LC['src_data'])}\" \"{os.path.join(LC['data_root'], LC['src_data'])}\"\n",
        "  !cp \"{os.path.join(LC['drive_root'], LC['src_testing'])}\" \"{os.path.join(LC['testing_root'], LC['src_testing'])}\"\n",
        "\n",
        "  !unzip -o \"{os.path.join(LC['data_root'], LC['src_data'])}\" -d \"{os.path.join(LC['data_root'])}\"\n",
        "  !unzip -o \"{os.path.join(LC['testing_root'], LC['src_testing'])}\" -d \"{os.path.join(LC['testing_root'])}\"\n",
        "  !rm \"{os.path.join(LC['data_root'], LC['src_data'])}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOyd8QaKEZj-"
      },
      "source": [
        "## Get Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DV6mCeNI-uS"
      },
      "outputs": [],
      "source": [
        "def getPairs(filename):\n",
        "    # Read the file and split into lines\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs\n",
        "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVdF5sisIaZs"
      },
      "source": [
        "## Get Testing File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XejdeiGgIaZt"
      },
      "outputs": [],
      "source": [
        "def getTestingFile(filename):\n",
        "    # Read the file and split into lines\n",
        "    content = open(filename, encoding='utf-8').read().strip()\n",
        "\n",
        "    return json.loads(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfsuXKo0EZj-"
      },
      "source": [
        "## Create Lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAk2ISpqEZj-"
      },
      "outputs": [],
      "source": [
        "def createLang(pairs, pairIndex, isIpa):\n",
        "    lang = Lang(isIpa)\n",
        "\n",
        "    for pair in pairs:\n",
        "        lang.addWord(pair[pairIndex])\n",
        "\n",
        "    return lang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkZZfrc6EZj_"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lMEO4XuEZj_"
      },
      "outputs": [],
      "source": [
        "def createModel():\n",
        "    config = copy.copy(LC['default_model_config'])\n",
        "    config['created_at'] = getDateTime()\n",
        "    config['lang'] = LC['lang']\n",
        "\n",
        "    LC['model_path'] = os.path.join(LC['model_root'], config['lang'], config['title'])\n",
        "    LC['model_export_path'] = os.path.join(LC['drive_root'], LC['model_root'], config['lang'], config['title'] + '.zip')\n",
        "\n",
        "    pairs = getPairs(os.path.join(LC['data_root'], config['lang'] + '.tsv'))\n",
        "\n",
        "    if LC['batch_size'] == 'auto':\n",
        "        LC['batch_size'] = len(pairs)\n",
        "\n",
        "    inputLang = createLang(pairs, 0, True)\n",
        "    outputLang = createLang(pairs, 1, False)\n",
        "\n",
        "    encoder = EncoderRNN(inputLang.n_tokens, config['hidden_size']).to(device)\n",
        "    decoder = AttnDecoderRNN(config['hidden_size'], outputLang.n_tokens, config['max_length']).to(device)\n",
        "    encoder = encoder.to(memory_format=torch.channels_last)\n",
        "    decoder = decoder.to(memory_format=torch.channels_last)\n",
        "\n",
        "    testing = getTestingFile(os.path.join(LC['testing_root'], 'testing_%s.json' % config['lang']))\n",
        "    config['testing_max_total'] = testing[2][len(testing[2]) - 1]\n",
        "    verifyData = getTestingFile(os.path.join(LC['testing_root'], 'verify_%s.json' % config['lang']))\n",
        "    config['verify_max_total'] = verifyData[2][len(verifyData[2]) - 1]\n",
        "\n",
        "    romanized = []\n",
        "\n",
        "    for item in testing[2]:\n",
        "      if not isinstance(item, numbers.Number):\n",
        "        romanized.append(romanize(config['lang'], item))\n",
        "\n",
        "    testing.append(romanized)\n",
        "\n",
        "    return {\n",
        "        'config': config,\n",
        "        'input_lang': inputLang,\n",
        "        'output_lang': outputLang,\n",
        "        'pairs': pairs,\n",
        "        'encoder': encoder,\n",
        "        'decoder': decoder,\n",
        "        'loss_plot': [],\n",
        "        'testing': testing,\n",
        "        'verify': verifyData\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em237Hc6EZj_"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiBv2dhIEZj_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def loadModel():\n",
        "    LC['model_export_path'] = os.path.join(LC['drive_root'], LC['model_root'], LC['lang'], LC['to_load_model'] + '.zip')\n",
        "    LC['model_path'] = os.path.join(LC['model_root'], LC['lang'], LC['to_load_model'])\n",
        "\n",
        "    !mkdir -p \"{LC['model_path']}\"\n",
        "    if LC['is_colab']:\n",
        "        !unzip -o \"{LC['model_export_path']}\" -d \"{LC['model_path']}\"\n",
        "\n",
        "    config = False\n",
        "    with open(os.path.join(LC['model_path'], 'config.json'), 'r') as f:\n",
        "        config = json.load(f)\n",
        "    LC['lang'] = config['lang']\n",
        "\n",
        "    loss_plot = False\n",
        "    with open(os.path.join(LC['model_path'], 'loss_plot.json'), 'r') as f:\n",
        "        loss_plot = json.load(f)\n",
        "\n",
        "    pairs = getPairs(os.path.join(LC['model_path'], 'pairs.tsv'))\n",
        "\n",
        "    inputLang = False\n",
        "    with open(os.path.join(LC['model_path'], 'input_lang.pickle'), 'rb') as f:\n",
        "        inputLang = pickle.load(f)\n",
        "\n",
        "    outputLang = False\n",
        "    with open(os.path.join(LC['model_path'], 'output_lang.pickle'), 'rb') as f:\n",
        "        outputLang = pickle.load(f)\n",
        "\n",
        "    encoder = EncoderRNN(inputLang.n_tokens, config['hidden_size']).to(device)\n",
        "    decoder = AttnDecoderRNN(config['hidden_size'], outputLang.n_tokens, config['max_length']).to(device)\n",
        "\n",
        "    encoder.load_state_dict(torch.load(os.path.join(LC['model_path'], 'encoder.pth')))\n",
        "    decoder.load_state_dict(torch.load(os.path.join(LC['model_path'], 'decoder.pth')))\n",
        "    encoder = encoder.to(memory_format=torch.channels_last)\n",
        "    decoder = decoder.to(memory_format=torch.channels_last)\n",
        "\n",
        "    testing = False\n",
        "    with open(os.path.join(LC['model_path'], 'testing.json')) as f:\n",
        "        testing = json.load(f)\n",
        "\n",
        "    LC['model']['config']['testing_max_total'] = testing[2][len(testing[2]) - 1]\n",
        "\n",
        "    verifyData = False\n",
        "    with open(os.path.join(LC['model_path'], 'verify.json')) as f:\n",
        "        verifyData = json.load(f)\n",
        "\n",
        "    LC['model']['config']['verify_max_total'] = verifyData[2][len(verifyData[2]) - 1]\n",
        "\n",
        "    return {\n",
        "        'config': config,\n",
        "        'input_lang': inputLang,\n",
        "        'output_lang': outputLang,\n",
        "        'pairs': pairs,\n",
        "        'encoder': encoder,\n",
        "        'decoder': decoder,\n",
        "        'loss_plot': loss_plot,\n",
        "        'testing': testing,\n",
        "        'verify': verifyData\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E73D6DAtEZj_"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz_A1RJlEZkA"
      },
      "outputs": [],
      "source": [
        "def saveModel():\n",
        "    print('Saving...')\n",
        "    !mkdir -p \"{LC['model_path']}\"\n",
        "\n",
        "    LC['model']['config']['testing_saved_total'] = LC['model']['config']['testing_latest_total']\n",
        "    LC['model']['config']['testing_saved_lv'] = LC['model']['config']['testing_latest_lv']\n",
        "    LC['model']['config']['verify_saved_total'] = LC['model']['config']['verify_latest_total']\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'config.json'), 'w') as f:\n",
        "        json.dump(LC['model']['config'], f)\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'loss_plot.json'), 'w') as f:\n",
        "        json.dump(LC['model']['loss_plot'], f)\n",
        "\n",
        "    lines = []\n",
        "    for pair in LC['model']['pairs']:\n",
        "        lines.append(\"\\t\".join(pair))\n",
        "    with open(os.path.join(LC['model_path'], 'pairs.tsv'), 'w') as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'input_lang.pickle'), 'wb') as f:\n",
        "        pickle.dump(LC['model']['input_lang'], f)\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'output_lang.pickle'), 'wb') as f:\n",
        "        pickle.dump(LC['model']['output_lang'], f)\n",
        "\n",
        "    torch.save(LC['model']['encoder'].state_dict(), os.path.join(LC['model_path'], 'encoder.pth'))\n",
        "    torch.save(LC['model']['decoder'].state_dict(), os.path.join(LC['model_path'], 'decoder.pth'))\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'testing.json'), 'w') as f:\n",
        "        json.dump(LC['model']['testing'], f)\n",
        "\n",
        "    with open(os.path.join(LC['model_path'], 'verify.json'), 'w') as f:\n",
        "        json.dump(LC['model']['verify'], f)\n",
        "\n",
        "    df = pd.DataFrame(LC['model']['testing'])\n",
        "    df.to_csv(os.path.join(LC['model_path'], 'testing.csv'), quoting=csv.QUOTE_NONNUMERIC)\n",
        "\n",
        "    df = pd.DataFrame(LC['model']['verify'])\n",
        "    df.to_csv(os.path.join(LC['model_path'], 'verify.csv'), quoting=csv.QUOTE_NONNUMERIC)\n",
        "\n",
        "    if LC['is_colab']:\n",
        "        !zip -q \"{LC['model_export_path']}\" -j \"{os.path.join('.', LC['model_path'])}\"/*\n",
        "\n",
        "    print('Saved!')\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkjmrV7aEZkA"
      },
      "source": [
        "## Standardise IPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGxerzZwEZkA"
      },
      "outputs": [],
      "source": [
        "def standardiseIpa(word):\n",
        "    return ''.join(tokenise(word, strict=False, replace=True, diphthongs=False, tones=False, unknown=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QooT_7bGEZkA"
      },
      "source": [
        "## Split Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itBY8ib9EZkA"
      },
      "outputs": [],
      "source": [
        "def splitInput(inputWord):\n",
        "    word = ''\n",
        "    language = False\n",
        "    if len(inputWord) > 2:\n",
        "        if inputWord[2] == \"_\":\n",
        "            language = inputWord[:3]\n",
        "            word = inputWord[3:]\n",
        "        else:\n",
        "            word = inputWord\n",
        "    else:\n",
        "        word = inputWord\n",
        "\n",
        "    return word, language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXauv9YtEZkB"
      },
      "source": [
        "## Tokenize Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM9zBBJlEZkB"
      },
      "outputs": [],
      "source": [
        "def tokenizeWord(lang, word):\n",
        "    tokens = []\n",
        "    if lang.isIpa:\n",
        "      word, language = splitInput(word)\n",
        "\n",
        "      if language:\n",
        "          tokens.append(language)\n",
        "\n",
        "      word = ''.join(tokenise(word, strict=False, replace=True, diphthongs=False, tones=False, unknown=False))\n",
        "    for token in word:\n",
        "        tokens.append(token)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QbGT71EZkB"
      },
      "source": [
        "## Get Indexes From Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqLiypl0EZkC"
      },
      "outputs": [],
      "source": [
        "def indexesFromWord(lang, word):\n",
        "    return [lang.token2index[token] for token in tokenizeWord(lang, word)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mB_R4Q4EZkC"
      },
      "source": [
        "## Get Tensor From Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEnFRlT3EZkC"
      },
      "outputs": [],
      "source": [
        "def tensorFromWord(lang, word):\n",
        "    indexes = indexesFromWord(lang, word)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETlZxqLWEZkQ"
      },
      "source": [
        "## Get Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n35BMmx_I-uW"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(input_lang, output_lang, pairs, batch_size, maxWordLength):\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, maxWordLength), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, maxWordLength), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromWord(input_lang, inp)\n",
        "        tgt_ids = indexesFromWord(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=2, persistent_workers=True)\n",
        "\n",
        "    return train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhXXvT8FEZkQ"
      },
      "source": [
        "## Train Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlunAKeKI-uW"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, scaler):\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad(set_to_none=True)\n",
        "        decoder_optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "          encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "          decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "          loss = criterion(\n",
        "              decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "              target_tensor.view(-1)\n",
        "          )\n",
        "        #autocast until here\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        scaler.step(encoder_optimizer)\n",
        "        scaler.step(decoder_optimizer)\n",
        "\n",
        "        scaler.update()\n",
        "#        loss.backward()\n",
        "\n",
        "#        encoder_optimizer.step()\n",
        "#        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Iej77AEZkQ"
      },
      "source": [
        "## Get Time as Minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOPmJeW4I-uX"
      },
      "outputs": [],
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-vMylRGEZkR"
      },
      "source": [
        "## Get Time Since"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNEhUwA1EZkR"
      },
      "outputs": [],
      "source": [
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN-TVa4feZSH"
      },
      "source": [
        "## Romanize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uobNMtOSebUY"
      },
      "outputs": [],
      "source": [
        "def romanize(language, word):\n",
        "    if language == 'zh':\n",
        "        return pinyin_jyutping_sentence.pinyin(word)\n",
        "    elif language == 'ja':\n",
        "        kks = pykakasi.kakasi()\n",
        "        conversionResult = kks.convert(word)\n",
        "        res = ''\n",
        "        for item in conversionResult:\n",
        "            res += item['hepburn']\n",
        "        return res\n",
        "    elif language == 'ko':\n",
        "        r = Romanizer(word)\n",
        "        return r.romanize()\n",
        "    else:\n",
        "        return word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv2A7mZFI-uY"
      },
      "source": [
        "## Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxnLPGPfI-uY"
      },
      "outputs": [],
      "source": [
        "def evaluate(word):\n",
        "    encoder = LC['model']['encoder']\n",
        "    decoder = LC['model']['decoder']\n",
        "    input_lang = LC['model']['input_lang']\n",
        "    output_lang = LC['model']['output_lang']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromWord(input_lang, word)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_tokens = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                break\n",
        "            decoded_tokens.append(output_lang.index2token[idx.item()])\n",
        "    return ''.join(decoded_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELpr0qaYEZkR"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewUKa--sI-uX"
      },
      "outputs": [],
      "source": [
        "def test(epoch):\n",
        "    testing = LC['model']['testing']\n",
        "\n",
        "    LC['model']['encoder'].eval()\n",
        "    LC['model']['decoder'].eval()\n",
        "\n",
        "    totals = {\n",
        "        'total': 0\n",
        "    }\n",
        "    totalsarr = []\n",
        "    res = []\n",
        "\n",
        "    for i in range(0, len(testing[0])):\n",
        "        if testing[0][i] != 'total':\n",
        "            testlang = testing[0][i]\n",
        "            testword = testing[1][i]\n",
        "            expected = testing[2][i]\n",
        "\n",
        "            actual = evaluate(testword)\n",
        "            if testlang not in totals:\n",
        "              totals[testlang] = 0\n",
        "            if actual == expected:\n",
        "                res.append('')\n",
        "                totals[testlang] += 1\n",
        "                totals['total'] += 1\n",
        "            else:\n",
        "                res.append(\"%s\\n(%s)\" % (actual, romanize(LC['lang'], actual)))\n",
        "        else:\n",
        "            if testing[1][i] != 'all':\n",
        "                testlang = testing[1][i]\n",
        "                res.append(totals[testlang])\n",
        "                totalsarr.append(totals[testlang])\n",
        "                if testlang == 'lv':\n",
        "                    LC['model']['config']['testing_latest_lv'] = totals[testlang]\n",
        "\n",
        "                    if totals[testlang] > LC['model']['config']['testing_high_lv']:\n",
        "                        LC['model']['config']['testing_high_lv'] = totals[testlang]\n",
        "            else:\n",
        "                res.append(totals['total'])\n",
        "    res.append(epoch)\n",
        "\n",
        "    LC['model']['testing'].append(res)\n",
        "    median = statistics.median(totalsarr)\n",
        "    # Consider moving this elsewhere\n",
        "    LC['model']['config']['testing_latest_total'] = totals['total']\n",
        "    LC['model']['config']['testing_latest_median'] = median\n",
        "    LC['model']['config']['testing_latest_epoch'] = epoch\n",
        "    if (totals['total'] > LC['model']['config']['testing_high_total']) or (totals['total'] == LC['model']['config']['testing_high_total'] and median > LC['model']['config']['testing_high_median']):\n",
        "        LC['model']['config']['testing_high_total'] = totals['total']\n",
        "        LC['model']['config']['testing_high_median'] = median\n",
        "        LC['model']['config']['testing_high_epoch'] = epoch\n",
        "\n",
        "    # Return to traning mode\n",
        "    LC['model']['encoder'].train()\n",
        "    LC['model']['decoder'].train()\n",
        "\n",
        "    return totals['total'], median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soJt6XMtRALG"
      },
      "source": [
        "## Verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thTuw2G9RALI"
      },
      "outputs": [],
      "source": [
        "def verify(epoch):\n",
        "    testing = LC['model']['verify']\n",
        "\n",
        "    LC['model']['encoder'].eval()\n",
        "    LC['model']['decoder'].eval()\n",
        "\n",
        "    totals = {\n",
        "        'total': 0\n",
        "    }\n",
        "    totalsarr = []\n",
        "    res = []\n",
        "\n",
        "    for i in range(0, len(testing[0])):\n",
        "        if testing[0][i] != 'total':\n",
        "            testlang = testing[0][i]\n",
        "            testword = testing[1][i]\n",
        "            expected = testing[2][i]\n",
        "\n",
        "            actual = evaluate(testword)\n",
        "            if testlang not in totals:\n",
        "              totals[testlang] = 0\n",
        "            if actual == expected:\n",
        "                res.append('')\n",
        "                totals[testlang] += 1\n",
        "                totals['total'] += 1\n",
        "            else:\n",
        "                res.append(\"%s\\n(%s)\" % (actual, romanize(LC['lang'], actual)))\n",
        "        else:\n",
        "            if testing[1][i] != 'all':\n",
        "                testlang = testing[1][i]\n",
        "                res.append(totals[testlang])\n",
        "                totalsarr.append(totals[testlang])\n",
        "            else:\n",
        "                res.append(totals['total'])\n",
        "    res.append(epoch)\n",
        "\n",
        "    LC['model']['verify'].append(res)\n",
        "    median = statistics.median(totalsarr)\n",
        "    # Consider moving this elsewhere\n",
        "    LC['model']['config']['verify_latest_total'] = totals['total']\n",
        "    LC['model']['config']['verify_latest_median'] = median\n",
        "    LC['model']['config']['verify_latest_epoch'] = epoch\n",
        "    if (totals['total'] > LC['model']['config']['verify_high_total']) or (totals['total'] == LC['model']['config']['verify_high_total'] and median > LC['model']['config']['verify_high_median']):\n",
        "        LC['model']['config']['verify_high_total'] = totals['total']\n",
        "        LC['model']['config']['verify_high_median'] = median\n",
        "        LC['model']['config']['verify_high_epoch'] = epoch\n",
        "\n",
        "    # Return to traning mode\n",
        "    LC['model']['encoder'].train()\n",
        "    LC['model']['decoder'].train()\n",
        "\n",
        "    return totals['total'], median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEtguqQXIaZ2"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcHhtELmIaZ3"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, test_every=50, plot_every=100):\n",
        "    start = time.time()\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, scaler)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        LC['model']['config']['epoch_count'] += 1\n",
        "\n",
        "        if epoch % test_every == 0:\n",
        "            totalsV, medianV = verify(epoch)\n",
        "            totalsT, medianT = test(epoch)\n",
        "            if totalsT == LC['model']['config']['testing_high_total']:\n",
        "                isHighScore = 'HIGH SCORE!'\n",
        "            else:\n",
        "                isHighScore = ''\n",
        "            print(\"%s %s total score %s/%s (%s), median %s. verify score %s/%s (%s), median %s.\" % (isHighScore, epoch, totalsT, LC['model']['config']['testing_max_total'], str(round(totalsT / LC['model']['config']['testing_max_total'] * 100, 2)) + '%', medianT, totalsV, LC['model']['config']['verify_max_total'], str(round(totalsV / LC['model']['config']['verify_max_total'] * 100, 2)) + '%', medianV))\n",
        "            if (totalsT == LC['model']['config']['testing_high_total'] and LC['to_save'] and (totalsT > LC['model']['config']['testing_saved_total'] or LC['model']['config']['testing_latest_lv'] > LC['model']['config']['testing_saved_lv'] or LC['model']['config']['verify_latest_total'] > LC['model']['config']['verify_saved_total'])):\n",
        "                LC['model']['config']['last_trained_at'] = getDateTime()\n",
        "                saveModel()\n",
        "            if totalsT == LC['model']['config']['testing_max_total']:\n",
        "                LC['to_save'] = False\n",
        "                break\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            LC['model']['loss_plot'].append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.8f' % (timeSince(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLbS1kBoEZkT"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXJSyx49EZkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d943ab5c-acb0-4a53-b7f2-334ffbdf01bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  data/training-v12-full.zip\n",
            "  inflating: data/ja.tsv             \n",
            "  inflating: data/ko.tsv             \n",
            "  inflating: data/zh.tsv             \n",
            "Archive:  testing/testing-v12.zip\n",
            "  inflating: testing/testing_ja.json  \n",
            "  inflating: testing/testing_ko.json  \n",
            "  inflating: testing/testing_zh.json  \n",
            "  inflating: testing/verify_ja.json  \n",
            "  inflating: testing/verify_ko.json  \n",
            "  inflating: testing/verify_zh.json  \n",
            "ko-v12-1280-full (ko):\n",
            "*  Input tokens: 99\n",
            "*  Output tokens: 1260\n",
            "*  Hidden size: 1280\n",
            "*  Max length: 17\n",
            "*  Epochs: 0\n",
            "*  Created at: 2024-05-05_09-39-23\n",
            "*  Last trained at: False\n",
            "*  Pairs: 112440\n"
          ]
        }
      ],
      "source": [
        "if not LC['is_initialized']:\n",
        "    %matplotlib inline\n",
        "\n",
        "    # Create directories\n",
        "    !mkdir -p \"{LC['data_root']}\" \"{LC['model_root']}\" \"{LC['testing_root']}\"\n",
        "\n",
        "    if LC['is_colab']:\n",
        "        from google.colab import drive\n",
        "        from google.colab import runtime\n",
        "        mountColab()\n",
        "\n",
        "    torch.multiprocessing.set_start_method('forkserver')\n",
        "\n",
        "    torch.autograd.set_detect_anomaly(False, check_nan=False)\n",
        "    torch.autograd.profiler.profile(enabled=False)\n",
        "    torch.autograd.profiler.emit_nvtx(enabled=False)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if LC['to_load']:\n",
        "        LC['model'] = loadModel()\n",
        "    else:\n",
        "        LC['model'] = createModel()\n",
        "\n",
        "    print(\"%s (%s):\" % (LC['model']['config']['title'], LC['model']['config']['lang']));\n",
        "    print(\"*  Input tokens: %s\" % LC['model']['input_lang'].n_tokens);\n",
        "    print(\"*  Output tokens: %s\" % LC['model']['output_lang'].n_tokens);\n",
        "    print(\"*  Hidden size: %s\" % LC['model']['config']['hidden_size']);\n",
        "    print(\"*  Max length: %s\" % LC['model']['config']['max_length']);\n",
        "    print(\"*  Epochs: %s\" % LC['model']['config']['epoch_count']);\n",
        "    print(\"*  Created at: %s\" % LC['model']['config']['created_at']);\n",
        "    print(\"*  Last trained at: %s\" % LC['model']['config']['last_trained_at']);\n",
        "    print(\"*  Pairs: %s\" % len(LC['model']['pairs']));\n",
        "\n",
        "    LC['is_initialized'] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdJtgce7I-uY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVbidY61I-uY",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2070a44f-2d5f-4fd0-c611-cd9ab15011ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs to train: 500\n",
            "HIGH SCORE! 5 total score 5/195 (2.56%), median 0. verify score 9/195 (4.62%), median 0.\n",
            "Saving...\n",
            "Saved!\n",
            "1m 17s (- 128m 24s) (5 1%) 0.79084790\n",
            "HIGH SCORE! 10 total score 15/195 (7.69%), median 1. verify score 23/195 (11.79%), median 1.\n",
            "Saving...\n",
            "Saved!\n",
            "2m 32s (- 124m 10s) (10 2%) 0.18125026\n",
            " 15 total score 11/195 (5.64%), median 0. verify score 23/195 (11.79%), median 1.\n",
            "3m 35s (- 116m 20s) (15 3%) 0.08902244\n",
            " 20 total score 10/195 (5.13%), median 0. verify score 24/195 (12.31%), median 1.\n",
            "4m 40s (- 112m 5s) (20 4%) 0.05421289\n",
            " 25 total score 8/195 (4.1%), median 0. verify score 27/195 (13.85%), median 2.\n",
            "5m 44s (- 109m 11s) (25 5%) 0.03398505\n",
            " 30 total score 10/195 (5.13%), median 0. verify score 26/195 (13.33%), median 2.\n",
            "6m 48s (- 106m 44s) (30 6%) 0.02157648\n",
            " 35 total score 14/195 (7.18%), median 1. verify score 30/195 (15.38%), median 2.\n",
            "7m 52s (- 104m 39s) (35 7%) 0.01397569\n",
            " 40 total score 13/195 (6.67%), median 1. verify score 31/195 (15.9%), median 3.\n",
            "8m 57s (- 103m 1s) (40 8%) 0.00876545\n",
            " 45 total score 10/195 (5.13%), median 0. verify score 26/195 (13.33%), median 2.\n",
            "10m 2s (- 101m 27s) (45 9%) 0.00590544\n",
            " 50 total score 10/195 (5.13%), median 0. verify score 28/195 (14.36%), median 3.\n",
            "11m 5s (- 99m 52s) (50 10%) 0.00419727\n",
            " 55 total score 11/195 (5.64%), median 1. verify score 22/195 (11.28%), median 2.\n",
            "12m 9s (- 98m 23s) (55 11%) 0.00267645\n",
            " 60 total score 10/195 (5.13%), median 1. verify score 31/195 (15.9%), median 3.\n",
            "13m 14s (- 97m 5s) (60 12%) 0.00182015\n",
            " 65 total score 11/195 (5.64%), median 1. verify score 27/195 (13.85%), median 2.\n",
            "14m 18s (- 95m 45s) (65 13%) 0.00149868\n",
            "HIGH SCORE! 70 total score 116/195 (59.49%), median 9. verify score 159/195 (81.54%), median 12.\n",
            "Saving...\n",
            "Saved!\n",
            "15m 32s (- 95m 25s) (70 14%) 0.14984608\n",
            "HIGH SCORE! 75 total score 118/195 (60.51%), median 9. verify score 174/195 (89.23%), median 14.\n",
            "Saving...\n",
            "Saved!\n",
            "16m 45s (- 94m 58s) (75 15%) 0.02255890\n",
            "HIGH SCORE! 80 total score 122/195 (62.56%), median 10. verify score 177/195 (90.77%), median 14.\n",
            "Saving...\n",
            "Saved!\n",
            "17m 59s (- 94m 25s) (80 16%) 0.00981092\n",
            " 85 total score 120/195 (61.54%), median 10. verify score 178/195 (91.28%), median 14.\n",
            "19m 2s (- 92m 59s) (85 17%) 0.00533285\n",
            "HIGH SCORE! 90 total score 123/195 (63.08%), median 10. verify score 184/195 (94.36%), median 14.\n",
            "Saving...\n",
            "Saved!\n",
            "20m 16s (- 92m 20s) (90 18%) 0.00338332\n",
            "HIGH SCORE! 95 total score 123/195 (63.08%), median 10. verify score 184/195 (94.36%), median 14.\n",
            "21m 20s (- 90m 56s) (95 19%) 0.00236829\n",
            "HIGH SCORE! 100 total score 124/195 (63.59%), median 10. verify score 182/195 (93.33%), median 14.\n",
            "Saving...\n",
            "Saved!\n",
            "22m 33s (- 90m 13s) (100 20%) 0.00194283\n",
            " 105 total score 121/195 (62.05%), median 9. verify score 181/195 (92.82%), median 14.\n",
            "23m 37s (- 88m 52s) (105 21%) 0.00150151\n",
            " 110 total score 122/195 (62.56%), median 10. verify score 182/195 (93.33%), median 14.\n",
            "24m 41s (- 87m 34s) (110 22%) 0.00118897\n",
            " 115 total score 120/195 (61.54%), median 9. verify score 183/195 (93.85%), median 14.\n",
            "25m 45s (- 86m 15s) (115 23%) 0.00101567\n",
            "HIGH SCORE! 120 total score 125/195 (64.1%), median 9. verify score 185/195 (94.87%), median 15.\n",
            "Saving...\n",
            "Saved!\n",
            "26m 59s (- 85m 28s) (120 24%) 0.00121703\n",
            " 125 total score 122/195 (62.56%), median 10. verify score 180/195 (92.31%), median 14.\n",
            "28m 3s (- 84m 9s) (125 25%) 0.00150571\n",
            "HIGH SCORE! 130 total score 129/195 (66.15%), median 11. verify score 183/195 (93.85%), median 14.\n",
            "Saving...\n",
            "Saved!\n",
            "29m 16s (- 83m 18s) (130 26%) 0.00128525\n",
            " 135 total score 125/195 (64.1%), median 9. verify score 185/195 (94.87%), median 15.\n",
            "30m 19s (- 82m 0s) (135 27%) 0.00080993\n",
            " 140 total score 128/195 (65.64%), median 10. verify score 183/195 (93.85%), median 14.\n",
            "31m 24s (- 80m 45s) (140 28%) 0.00061341\n",
            " 145 total score 128/195 (65.64%), median 10. verify score 186/195 (95.38%), median 14.\n",
            "32m 28s (- 79m 31s) (145 28%) 0.00054533\n",
            " 150 total score 126/195 (64.62%), median 9. verify score 184/195 (94.36%), median 14.\n",
            "33m 32s (- 78m 16s) (150 30%) 0.00052684\n",
            " 155 total score 115/195 (58.97%), median 9. verify score 171/195 (87.69%), median 14.\n",
            "34m 36s (- 77m 2s) (155 31%) 0.00185645\n",
            " 160 total score 126/195 (64.62%), median 10. verify score 184/195 (94.36%), median 14.\n",
            "35m 40s (- 75m 48s) (160 32%) 0.00418470\n",
            "HIGH SCORE! 165 total score 129/195 (66.15%), median 10. verify score 186/195 (95.38%), median 14.\n",
            "Saving...\n",
            "Saved!\n",
            "36m 54s (- 74m 55s) (165 33%) 0.00091263\n",
            " 170 total score 127/195 (65.13%), median 10. verify score 187/195 (95.9%), median 14.\n",
            "37m 58s (- 73m 42s) (170 34%) 0.00053366\n",
            " 175 total score 128/195 (65.64%), median 10. verify score 189/195 (96.92%), median 15.\n",
            "39m 2s (- 72m 31s) (175 35%) 0.00045497\n",
            "HIGH SCORE! 180 total score 129/195 (66.15%), median 10. verify score 185/195 (94.87%), median 14.\n",
            "40m 6s (- 71m 18s) (180 36%) 0.00042036\n",
            "HIGH SCORE! 185 total score 132/195 (67.69%), median 10. verify score 185/195 (94.87%), median 14.\n",
            "Saving...\n",
            "Saved!\n",
            "41m 20s (- 70m 23s) (185 37%) 0.00039930\n",
            " 190 total score 130/195 (66.67%), median 10. verify score 187/195 (95.9%), median 15.\n",
            "42m 24s (- 69m 10s) (190 38%) 0.00035361\n",
            " 195 total score 130/195 (66.67%), median 10. verify score 188/195 (96.41%), median 15.\n",
            "43m 28s (- 67m 59s) (195 39%) 0.00035199\n",
            " 200 total score 129/195 (66.15%), median 10. verify score 186/195 (95.38%), median 15.\n",
            "44m 32s (- 66m 48s) (200 40%) 0.00034473\n",
            " 205 total score 128/195 (65.64%), median 10. verify score 185/195 (94.87%), median 14.\n",
            "45m 36s (- 65m 37s) (205 41%) 0.00032731\n",
            " 210 total score 122/195 (62.56%), median 10. verify score 178/195 (91.28%), median 14.\n",
            "46m 40s (- 64m 26s) (210 42%) 0.00109011\n",
            " 215 total score 111/195 (56.92%), median 8. verify score 172/195 (88.21%), median 13.\n",
            "47m 44s (- 63m 17s) (215 43%) 0.01446025\n",
            " 220 total score 122/195 (62.56%), median 9. verify score 187/195 (95.9%), median 15.\n",
            "48m 48s (- 62m 7s) (220 44%) 0.00296102\n",
            " 225 total score 122/195 (62.56%), median 9. verify score 190/195 (97.44%), median 15.\n",
            "49m 52s (- 60m 57s) (225 45%) 0.00062717\n",
            " 230 total score 125/195 (64.1%), median 10. verify score 191/195 (97.95%), median 15.\n",
            "50m 56s (- 59m 48s) (230 46%) 0.00046765\n",
            " 235 total score 126/195 (64.62%), median 10. verify score 191/195 (97.95%), median 15.\n",
            "52m 0s (- 58m 38s) (235 47%) 0.00040880\n",
            " 240 total score 127/195 (65.13%), median 10. verify score 191/195 (97.95%), median 15.\n",
            "53m 3s (- 57m 29s) (240 48%) 0.00037374\n",
            " 245 total score 124/195 (63.59%), median 10. verify score 191/195 (97.95%), median 15.\n",
            "54m 8s (- 56m 21s) (245 49%) 0.00035038\n",
            " 250 total score 123/195 (63.08%), median 9. verify score 191/195 (97.95%), median 15.\n",
            "55m 11s (- 55m 11s) (250 50%) 0.00033269\n",
            " 255 total score 125/195 (64.1%), median 10. verify score 192/195 (98.46%), median 15.\n",
            "56m 15s (- 54m 3s) (255 51%) 0.00030997\n",
            " 260 total score 126/195 (64.62%), median 10. verify score 191/195 (97.95%), median 15.\n",
            "57m 20s (- 52m 55s) (260 52%) 0.00031599\n",
            " 265 total score 126/195 (64.62%), median 10. verify score 191/195 (97.95%), median 15.\n",
            "58m 23s (- 51m 47s) (265 53%) 0.00030936\n",
            " 270 total score 124/195 (63.59%), median 10. verify score 190/195 (97.44%), median 15.\n",
            "59m 28s (- 50m 39s) (270 54%) 0.00030233\n",
            " 275 total score 125/195 (64.1%), median 10. verify score 189/195 (96.92%), median 15.\n",
            "60m 32s (- 49m 31s) (275 55%) 0.00030340\n",
            " 280 total score 125/195 (64.1%), median 10. verify score 189/195 (96.92%), median 15.\n",
            "61m 35s (- 48m 23s) (280 56%) 0.00029505\n",
            " 285 total score 127/195 (65.13%), median 10. verify score 188/195 (96.41%), median 15.\n",
            "62m 40s (- 47m 16s) (285 56%) 0.00028607\n",
            " 290 total score 123/195 (63.08%), median 10. verify score 190/195 (97.44%), median 15.\n",
            "63m 44s (- 46m 9s) (290 57%) 0.00028722\n",
            " 295 total score 125/195 (64.1%), median 10. verify score 189/195 (96.92%), median 15.\n",
            "64m 47s (- 45m 1s) (295 59%) 0.00028098\n",
            " 300 total score 100/195 (51.28%), median 8. verify score 148/195 (75.9%), median 11.\n",
            "65m 52s (- 43m 54s) (300 60%) 0.01188715\n",
            " 305 total score 115/195 (58.97%), median 9. verify score 178/195 (91.28%), median 14.\n",
            "66m 56s (- 42m 47s) (305 61%) 0.01461384\n",
            " 310 total score 120/195 (61.54%), median 10. verify score 186/195 (95.38%), median 15.\n",
            "68m 0s (- 41m 40s) (310 62%) 0.00130337\n",
            " 315 total score 119/195 (61.03%), median 10. verify score 188/195 (96.41%), median 15.\n",
            "69m 4s (- 40m 33s) (315 63%) 0.00053600\n",
            " 320 total score 119/195 (61.03%), median 10. verify score 190/195 (97.44%), median 15.\n",
            "70m 8s (- 39m 27s) (320 64%) 0.00043946\n",
            " 325 total score 119/195 (61.03%), median 10. verify score 190/195 (97.44%), median 15.\n",
            "71m 12s (- 38m 20s) (325 65%) 0.00038598\n",
            " 330 total score 119/195 (61.03%), median 10. verify score 190/195 (97.44%), median 15.\n",
            "72m 15s (- 37m 13s) (330 66%) 0.00036522\n",
            " 335 total score 121/195 (62.05%), median 10. verify score 191/195 (97.95%), median 15.\n",
            "73m 20s (- 36m 7s) (335 67%) 0.00033029\n",
            " 340 total score 119/195 (61.03%), median 10. verify score 193/195 (98.97%), median 15.\n",
            "74m 23s (- 35m 0s) (340 68%) 0.00031480\n",
            " 345 total score 120/195 (61.54%), median 10. verify score 190/195 (97.44%), median 15.\n",
            "75m 27s (- 33m 54s) (345 69%) 0.00030480\n",
            " 350 total score 121/195 (62.05%), median 10. verify score 192/195 (98.46%), median 15.\n",
            "76m 32s (- 32m 48s) (350 70%) 0.00030232\n",
            " 355 total score 118/195 (60.51%), median 10. verify score 191/195 (97.95%), median 15.\n",
            "77m 35s (- 31m 41s) (355 71%) 0.00028996\n",
            " 360 total score 120/195 (61.54%), median 10. verify score 192/195 (98.46%), median 15.\n",
            "78m 39s (- 30m 35s) (360 72%) 0.00029139\n",
            " 365 total score 121/195 (62.05%), median 10. verify score 192/195 (98.46%), median 15.\n",
            "79m 43s (- 29m 29s) (365 73%) 0.00028470\n",
            " 370 total score 120/195 (61.54%), median 10. verify score 193/195 (98.97%), median 15.\n",
            "80m 47s (- 28m 23s) (370 74%) 0.00027974\n",
            " 375 total score 117/195 (60.0%), median 9. verify score 192/195 (98.46%), median 15.\n",
            "81m 50s (- 27m 16s) (375 75%) 0.00028070\n",
            " 380 total score 121/195 (62.05%), median 10. verify score 192/195 (98.46%), median 15.\n",
            "82m 55s (- 26m 11s) (380 76%) 0.00027100\n",
            " 385 total score 117/195 (60.0%), median 9. verify score 192/195 (98.46%), median 15.\n",
            "83m 58s (- 25m 5s) (385 77%) 0.00028604\n",
            " 390 total score 120/195 (61.54%), median 10. verify score 192/195 (98.46%), median 15.\n",
            "85m 2s (- 23m 59s) (390 78%) 0.00027217\n",
            " 395 total score 111/195 (56.92%), median 8. verify score 155/195 (79.49%), median 13.\n",
            "86m 6s (- 22m 53s) (395 79%) 0.00350828\n",
            " 400 total score 112/195 (57.44%), median 9. verify score 177/195 (90.77%), median 14.\n",
            "87m 10s (- 21m 47s) (400 80%) 0.02015484\n",
            " 405 total score 116/195 (59.49%), median 9. verify score 185/195 (94.87%), median 14.\n",
            "88m 14s (- 20m 41s) (405 81%) 0.00240288\n",
            " 410 total score 121/195 (62.05%), median 9. verify score 187/195 (95.9%), median 15.\n",
            "89m 18s (- 19m 36s) (410 82%) 0.00059057\n",
            " 415 total score 121/195 (62.05%), median 9. verify score 189/195 (96.92%), median 15.\n",
            "90m 22s (- 18m 30s) (415 83%) 0.00043874\n",
            " 420 total score 123/195 (63.08%), median 9. verify score 189/195 (96.92%), median 15.\n",
            "91m 26s (- 17m 24s) (420 84%) 0.00037613\n",
            " 425 total score 123/195 (63.08%), median 10. verify score 188/195 (96.41%), median 15.\n",
            "92m 30s (- 16m 19s) (425 85%) 0.00034517\n",
            " 430 total score 125/195 (64.1%), median 10. verify score 189/195 (96.92%), median 15.\n",
            "93m 34s (- 15m 13s) (430 86%) 0.00032392\n",
            " 435 total score 124/195 (63.59%), median 10. verify score 188/195 (96.41%), median 15.\n",
            "94m 37s (- 14m 8s) (435 87%) 0.00031721\n",
            " 440 total score 125/195 (64.1%), median 10. verify score 189/195 (96.92%), median 15.\n",
            "95m 42s (- 13m 3s) (440 88%) 0.00031513\n",
            " 445 total score 125/195 (64.1%), median 10. verify score 190/195 (97.44%), median 15.\n",
            "96m 46s (- 11m 57s) (445 89%) 0.00030093\n",
            " 450 total score 126/195 (64.62%), median 10. verify score 188/195 (96.41%), median 15.\n",
            "97m 49s (- 10m 52s) (450 90%) 0.00028856\n",
            " 455 total score 126/195 (64.62%), median 10. verify score 189/195 (96.92%), median 15.\n",
            "98m 53s (- 9m 46s) (455 91%) 0.00028342\n",
            " 460 total score 121/195 (62.05%), median 9. verify score 188/195 (96.41%), median 15.\n",
            "99m 57s (- 8m 41s) (460 92%) 0.00032748\n",
            " 465 total score 119/195 (61.03%), median 10. verify score 189/195 (96.92%), median 15.\n",
            "101m 1s (- 7m 36s) (465 93%) 0.00031438\n",
            " 470 total score 118/195 (60.51%), median 9. verify score 189/195 (96.92%), median 15.\n",
            "102m 5s (- 6m 30s) (470 94%) 0.00034760\n",
            " 475 total score 119/195 (61.03%), median 9. verify score 189/195 (96.92%), median 15.\n",
            "103m 10s (- 5m 25s) (475 95%) 0.00030149\n",
            " 480 total score 119/195 (61.03%), median 10. verify score 190/195 (97.44%), median 15.\n",
            "104m 13s (- 4m 20s) (480 96%) 0.00028569\n",
            " 485 total score 120/195 (61.54%), median 10. verify score 189/195 (96.92%), median 15.\n",
            "105m 17s (- 3m 15s) (485 97%) 0.00027671\n",
            " 490 total score 119/195 (61.03%), median 10. verify score 189/195 (96.92%), median 15.\n",
            "106m 22s (- 2m 10s) (490 98%) 0.00027618\n",
            " 495 total score 121/195 (62.05%), median 10. verify score 191/195 (97.95%), median 15.\n",
            "107m 25s (- 1m 5s) (495 99%) 0.00027824\n",
            " 500 total score 116/195 (59.49%), median 10. verify score 185/195 (94.87%), median 14.\n",
            "108m 29s (- 0m 0s) (500 100%) 0.00044455\n"
          ]
        }
      ],
      "source": [
        "if LC['to_train']:\n",
        "    LC['model']['encoder'].train()\n",
        "    LC['model']['decoder'].train()\n",
        "    LC['model']['config']['last_trained_at'] = getDateTime()\n",
        "\n",
        "    train_dataloader = get_dataloader(\n",
        "        LC['model']['input_lang'],\n",
        "        LC['model']['output_lang'],\n",
        "        LC['model']['pairs'],\n",
        "        LC['batch_size'],\n",
        "        LC['model']['config']['max_length']\n",
        "    )\n",
        "\n",
        "    for inputs, targets in train_dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    epoch_count = LC['epoch_goal'] - LC['model']['config']['epoch_count']\n",
        "    print(\"Epochs to train: %s\" % epoch_count)\n",
        "    train(\n",
        "        train_dataloader,\n",
        "        LC['model']['encoder'],\n",
        "        LC['model']['decoder'],\n",
        "        epoch_count,\n",
        "        # print_every=math.floor(epoch_count/100*1),\n",
        "        print_every=5,\n",
        "        test_every=5,\n",
        "        # learning_rate=0.00075,\n",
        "        plot_every=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsKbeX2jI-ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ecf5a91-f3fd-423f-a368-088b861dc8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n",
            "Saved!\n"
          ]
        }
      ],
      "source": [
        "if LC['to_save']:\n",
        "    if LC['model']['config']['testing_max_total'] != LC['model']['config']['testing_latest_total']:\n",
        "      LC['model_path'] = os.path.join(LC['model_root'], LC['model']['config']['lang'], LC['model']['config']['title'] + '-' + str(LC['model']['config']['epoch_count']))\n",
        "      LC['model_export_path'] = os.path.join(LC['drive_root'], LC['model_root'], LC['model']['config']['lang'], LC['model']['config']['title'] + '-' + str(LC['model']['config']['epoch_count']) + '.zip')\n",
        "      saveModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv3LF9kCEZkV"
      },
      "source": [
        "# Exit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd4iSHnR0N6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f9e848-8dfd-4feb-c817-12fb5beb8c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  5 11:28:16 AM UTC 2024\n"
          ]
        }
      ],
      "source": [
        "!date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBilzGreJJh9"
      },
      "outputs": [],
      "source": [
        "if LC['is_colab'] and LC['colab_auto_quit']:\n",
        "  runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "x2R2Za-nEZj8"
      ],
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}